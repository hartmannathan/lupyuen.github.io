<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="rustdoc">
    <title>Machine Learning on RISC-V BL602 with TensorFlow Lite</title>

    
    <!-- Begin scripts/articles/*-header.html: Article Header for Custom Markdown files processed by rustdoc, like chip8.md -->
<meta property="og:title" 
    content="Machine Learning on RISC-V BL602 with TensorFlow Lite" 
    data-rh="true">
<meta property="og:description" 
    content="How we run TensorFlow Lite on RISC-V BL602... To create a Glowing LED"
    data-rh="true">
<meta property="og:image" 
    content="https://lupyuen.github.io/images/tflite-title.jpg">
<meta property="og:type" 
    content="article" data-rh="true">
<!-- End scripts/articles/*-header.html -->
<!-- Begin scripts/rustdoc-header.html: Header for Custom Markdown files processed by rustdoc, like chip8.md -->
<link rel="alternate" type="application/rss+xml" title="RSS Feed for lupyuen" href="/rss.xml" />
<link rel="stylesheet" type="text/css" href="../normalize.css">
<link rel="stylesheet" type="text/css" href="../rustdoc.css" id="mainThemeStyle">
<link rel="stylesheet" type="text/css" href="../dark.css">
<link rel="stylesheet" type="text/css" href="../light.css" id="themeStyle">
<link rel="stylesheet" type="text/css" href="../prism.css">
<script src="../storage.js"></script><noscript>
<link rel="stylesheet" href="../noscript.css"></noscript>
<link rel="shortcut icon" href="../favicon.ico">
<style type="text/css">
    #crate-search {
        background-image: url("../down-arrow.svg");
    }
    a {
        color: #77d;
    }
</style>
<!-- End scripts/rustdoc-header.html -->


</head>
<body class="rustdoc">
    <!--[if lte IE 8]>
    <div class="warning">
        This old browser is unsupported and will most likely display funky
        things.
    </div>
    <![endif]-->

        <!-- Begin scripts/rustdoc-before.html: Pre-HTML for Custom Markdown files processed by rustdoc, like chip8.md -->

    <!-- Begin Theme Picker -->
    <div class="theme-picker" style="left: 0"><button id="theme-picker" aria-label="Pick another theme!"><img src="../brush.svg"
        width="18" alt="Pick another theme!"></button>
        <div id="theme-choices"></div>
    </div>
    <script src="../theme.js"></script>
    <script src="../prism.js"></script>
    <!-- Theme Picker -->

    <!-- End scripts/rustdoc-before.html -->
    

    <h1 class="title">Machine Learning on RISC-V BL602 with TensorFlow Lite</h1>
    <nav id="TOC"><ul>
<li><a href="#tensorflow-lite-library">1 TensorFlow Lite Library</a><ul></ul></li>
<li><a href="#tensorflow-lite-firmware">2 TensorFlow Lite Firmware</a><ul>
<li><a href="#build-the-firmware">2.1 Build the Firmware</a><ul></ul></li>
<li><a href="#flash-the-firmware">2.2 Flash the Firmware</a><ul></ul></li>
<li><a href="#run-the-firmware">2.3 Run the Firmware</a><ul></ul></li></ul></li>
<li><a href="#machine-learning-in-action">3 Machine Learning in Action</a><ul>
<li><a href="#load-the-model">3.1 Load the Model</a><ul></ul></li>
<li><a href="#run-an-inference">3.2 Run an Inference</a><ul></ul></li></ul></li>
<li><a href="#how-accurate-is-it">4 How Accurate Is It?</a><ul></ul></li>
<li><a href="#how-it-works">5 How It Works</a><ul></ul></li>
<li><a href="#load-tensorflow-model">6 Load TensorFlow Model</a><ul></ul></li>
<li><a href="#run-tensorflow-inference">7 Run TensorFlow Inference</a><ul></ul></li>
<li><a href="#glow-the-led">8 Glow The LED</a><ul></ul></li>
<li><a href="#glowing-machine-learning-in-action">9 Glowing Machine Learning in Action</a><ul></ul></li>
<li><a href="#train-tensorflow-model">10 Train TensorFlow Model</a><ul></ul></li>
<li><a href="#what-else-can-tensorflow-do">11 What Else Can TensorFlow Do?</a><ul></ul></li>
<li><a href="#whats-next">12 What‚Äôs Next</a><ul></ul></li>
<li><a href="#notes">13 Notes</a><ul></ul></li>
<li><a href="#appendix-porting-tensorflow-to-bl602">14 Appendix: Porting TensorFlow to BL602</a><ul>
<li><a href="#source-repositories">14.1 Source Repositories</a><ul></ul></li>
<li><a href="#makefiles">14.2 Makefiles</a><ul></ul></li>
<li><a href="#source-folders">14.3 Source Folders</a><ul></ul></li>
<li><a href="#compiler-flags">14.4 Compiler Flags</a><ul></ul></li>
<li><a href="#download-libraries">14.5 Download Libraries</a><ul></ul></li>
<li><a href="#but-not-on-windows-msys">14.6 But Not On Windows MSYS</a><ul></ul></li>
<li><a href="#global-destructor">14.7 Global Destructor</a><ul></ul></li>
<li><a href="#math-overflow">14.8 Math Overflow</a><ul></ul></li>
<li><a href="#excluded-files">14.9 Excluded Files</a><ul></ul></li>
<li><a href="#optimise-tensorflow">14.10 Optimise TensorFlow</a><ul></ul></li></ul></li></ul></nav><p>üìù <em>22 Jun 2021</em></p>
<p>How a Human teaches a Machine to light up an LED‚Ä¶</p>
<blockquote>
<p><em>Human:</em> Hello Machine, please light up the LED in a fun and interesting way.</p>
</blockquote>
<blockquote>
<p><em>Machine:</em> OK I shall light up the LED: on - off - on -off - on - off‚Ä¶</p>
</blockquote>
<p><img src="https://lupyuen.github.io/images/tflite-chart1.jpg" alt="On - Off - On - Off" /></p>
<blockquote>
<p><em>Human:</em> That‚Äôs not very fun and interesting.</p>
</blockquote>
<blockquote>
<p><em>Machine:</em> OK Hooman‚Ä¶ Define fun and interesting.</p>
</blockquote>
<blockquote>
<p><em>Human:</em> Make the LED glow gently brighter and dimmer, brighter and dimmer, and so on.</p>
</blockquote>
<blockquote>
<p><em>Machine:</em> Like a wavy curve? Please teach me to draw a wavy curve.</p>
</blockquote>
<blockquote>
<p><em>Human:</em> Like this‚Ä¶</p>
</blockquote>
<p><img src="https://lupyuen.github.io/images/tflite-chart2.jpg" alt="Wavy Curve" /></p>
<blockquote>
<p><em>Machine:</em> OK I have been trained. I shall now use my trained model to infer the values of the wavy curve. And light up the LED in a fun and interesting way.</p>
</blockquote>
<ul>
<li><a href="https://youtu.be/EFpYJ3qsmEY"><strong>Watch the Demo Video on YouTube</strong></a></li>
</ul>
<p>This sounds like Science Fiction‚Ä¶ But <strong>this is possible today!</strong></p>
<p>(Except for the polite banter)</p>
<p>Read on to learn how <strong>Machine Learning (TensorFlow Lite)</strong> makes this possible on the <a href="https://lupyuen.github.io/articles/pinecone"><strong>BL602 RISC-V + WiFi SoC</strong></a>.</p>
<h1 id="tensorflow-lite-library"><a href="#tensorflow-lite-library">1 TensorFlow Lite Library</a></h1>
<p>Remember in our story‚Ä¶</p>
<ol>
<li>
<p>Our Machine <strong>learns to draw a wavy curve</strong></p>
</li>
<li>
<p>Our Machine <strong>reproduces the wavy curve</strong> (to light up the LED)</p>
</li>
</ol>
<p>To accomplish (1) and (2) on BL602, we shall use an open-source <strong>Machine Learning</strong> library: <a href="https://www.tensorflow.org/lite/microcontrollers"><strong>TensorFlow Lite for Microcontrollers</strong></a></p>
<p><em>What‚Äôs a Tensor?</em></p>
<p>Remember these from our Math Textbook? <strong>Scalar, Vector and Matrix</strong></p>
<p><img src="https://lupyuen.github.io/images/tflite-matrix.png" alt="Scalar, Vector, Matrix" /></p>
<p><a href="https://www.tensorflow.org/guide/tensor">(From TensorFlow Guide)</a></p>
<p>When we extend a Matrix from 2D to 3D, we get a <strong>Tensor With 3 Axes</strong>‚Ä¶</p>
<p><img src="https://lupyuen.github.io/images/tflite-tensor.png" alt="Tensor with 3 and 4 Axes" /></p>
<p>And yes we can have a <strong>Tensor With 4 or More Axes</strong>!</p>
<p><strong>Tensors With Multiple Dimensions</strong> are really useful for crunching the numbers needed for Machine Learning.</p>
<p>That‚Äôs how the TensorFlow library works: <strong>Computing lots of Tensors</strong>.</p>
<p>(Fortunately we won‚Äôt need to compute any Tensors ourselves‚Ä¶ The library does everything for us)</p>
<p><a href="https://www.tensorflow.org/guide/tensor">More about Tensors</a></p>
<p><em>Why is the library named TensorFlow?</em></p>
<p>Because it doesn‚Äôt drip, it flows üòÇ</p>
<p>But seriously‚Ä¶ In Machine Learning we push lots of numbers <strong>(Tensors)</strong> through various math functions over specific paths <strong>(Dataflow Graphs)</strong>.</p>
<p>That‚Äôs why it‚Äôs named <strong>‚ÄúTensorFlow‚Äù</strong></p>
<p>(Yes it sounds like the Neural Network in our brain)</p>
<p><a href="https://en.m.wikipedia.org/wiki/TensorFlow">More about TensorFlow</a></p>
<p><em>What‚Äôs the ‚ÄúLite‚Äù version of TensorFlow?</em></p>
<p>TensorFlow normally runs on powerful servers to perform Machine Learning tasks. (Like Speech Recognition and Image Recognition)</p>
<p>We‚Äôre using <strong>TensorFlow Lite</strong>, which is <strong>optimised for microcontrollers</strong>‚Ä¶</p>
<ol>
<li>
<p>Works on microcontrollers with <strong>limited RAM</strong></p>
<p>(Including Arduino, Arm and ESP32)</p>
</li>
<li>
<p>Uses <strong>Static Memory</strong> instead of Dynamic Memory (Heap)</p>
</li>
<li>
<p>But it only supports <strong>Basic Models</strong> of Machine Learning</p>
</li>
</ol>
<p>Today we shall study the TensorFlow Lite library that has been ported to BL602‚Ä¶</p>
<ul>
<li><a href="https://github.com/lupyuen/tflite-bl602"><strong><code>tflite-bl602</code> TensorFlow Lite Library for BL602</strong></a></li>
</ul>
<h1 id="tensorflow-lite-firmware"><a href="#tensorflow-lite-firmware">2 TensorFlow Lite Firmware</a></h1>
<p>Let‚Äôs build, flash and run the TensorFlow Lite Firmware for BL602‚Ä¶ And watch Machine Learning in action!</p>
<h2 id="build-the-firmware"><a href="#build-the-firmware">2.1 Build the Firmware</a></h2>
<p>Download the Firmware Binary File <strong><code>sdk_app_tflite.bin</code></strong> from‚Ä¶</p>
<ul>
<li><a href="https://github.com/lupyuen/bl_iot_sdk/releases/tag/v10.0.0"><strong>Binary Release of <code>sdk_app_tflite</code></strong></a></li>
</ul>
<p>Alternatively, we may build the Firmware Binary File <code>sdk_app_tflite.bin</code> from the <a href="https://github.com/lupyuen/bl_iot_sdk/tree/master/customer_app/sdk_app_tflite">source code</a>‚Ä¶</p>
<div class="example-wrap"><pre class="language-bash"><code># Download the master branch of lupyuen&#39;s bl_iot_sdk
git clone --recursive --branch master https://github.com/lupyuen/bl_iot_sdk

# TODO: Change this to the full path of bl_iot_sdk
export BL60X_SDK_PATH=$PWD/bl_iot_sdk
export CONFIG_CHIP_NAME=BL602

# Build the firmware
cd bl_iot_sdk/customer_app/sdk_app_tflite
make

# For WSL: Copy the firmware to /mnt/c/blflash, which refers to c:\blflash in Windows
mkdir /mnt/c/blflash
cp build_out/sdk_app_tflite.bin /mnt/c/blflash</code></pre></div>
<p><a href="https://lupyuen.github.io/articles/pinecone#building-firmware">More details on building bl_iot_sdk</a></p>
<h2 id="flash-the-firmware"><a href="#flash-the-firmware">2.2 Flash the Firmware</a></h2>
<p>Follow these steps to install <code>blflash</code>‚Ä¶</p>
<ol>
<li>
<p><a href="https://lupyuen.github.io/articles/flash#install-rustup"><strong>‚ÄúInstall rustup‚Äù</strong></a></p>
</li>
<li>
<p><a href="https://lupyuen.github.io/articles/flash#download-and-build-blflash"><strong>‚ÄúDownload and build blflash‚Äù</strong></a></p>
</li>
</ol>
<p>We assume that our Firmware Binary File <code>sdk_app_tflite.bin</code> has been copied to the <code>blflash</code> folder.</p>
<p>Set BL602 to <strong>Flashing Mode</strong> and restart the board‚Ä¶</p>
<p><strong>For PineCone:</strong></p>
<ol>
<li>
<p>Set the <strong>PineCone Jumper (IO 8)</strong> to the <strong><code>H</code> Position</strong> <a href="https://lupyuen.github.io/images/pinecone-jumperh.jpg">(Like this)</a></p>
</li>
<li>
<p>Press the Reset Button</p>
</li>
</ol>
<p><strong>For BL10:</strong></p>
<ol>
<li>
<p>Connect BL10 to the USB port</p>
</li>
<li>
<p>Press and hold the <strong>D8 Button (GPIO 8)</strong></p>
</li>
<li>
<p>Press and release the <strong>EN Button (Reset)</strong></p>
</li>
<li>
<p>Release the D8 Button</p>
</li>
</ol>
<p><strong>For <a href="https://docs.ai-thinker.com/en/wb2">Ai-Thinker Ai-WB2</a>, Pinenut and MagicHome BL602:</strong></p>
<ol>
<li>
<p>Disconnect the board from the USB Port</p>
</li>
<li>
<p>Connect <strong>GPIO 8</strong> to <strong>3.3V</strong></p>
</li>
<li>
<p>Reconnect the board to the USB port</p>
</li>
</ol>
<p>Enter these commands to flash <code>sdk_app_tflite.bin</code> to BL602 over UART‚Ä¶</p>
<div class="example-wrap"><pre class="language-bash"><code># For Linux:
blflash flash build_out/sdk_app_tflite.bin \
    --port /dev/ttyUSB0

# For macOS:
blflash flash build_out/sdk_app_tflite.bin \
    --port /dev/tty.usbserial-1420 \
    --initial-baud-rate 230400 \
    --baud-rate 230400

# For Windows: Change COM5 to the BL602 Serial Port
blflash flash c:\blflash\sdk_app_tflite.bin --port COM5</code></pre></div>
<p>(For WSL: Do this under plain old Windows CMD, not WSL, because blflash needs to access the COM port)</p>
<p><a href="https://lupyuen.github.io/articles/flash#flash-the-firmware">More details on flashing firmware</a></p>
<h2 id="run-the-firmware"><a href="#run-the-firmware">2.3 Run the Firmware</a></h2>
<p>Set BL602 to <strong>Normal Mode</strong> (Non-Flashing) and restart the board‚Ä¶</p>
<p><strong>For PineCone:</strong></p>
<ol>
<li>
<p>Set the <strong>PineCone Jumper (IO 8)</strong> to the <strong><code>L</code> Position</strong> <a href="https://lupyuen.github.io/images/pinecone-jumperl.jpg">(Like this)</a></p>
</li>
<li>
<p>Press the Reset Button</p>
</li>
</ol>
<p><strong>For BL10:</strong></p>
<ol>
<li>Press and release the <strong>EN Button (Reset)</strong></li>
</ol>
<p><strong>For <a href="https://docs.ai-thinker.com/en/wb2">Ai-Thinker Ai-WB2</a>, Pinenut and MagicHome BL602:</strong></p>
<ol>
<li>
<p>Disconnect the board from the USB Port</p>
</li>
<li>
<p>Connect <strong>GPIO 8</strong> to <strong>GND</strong></p>
</li>
<li>
<p>Reconnect the board to the USB port</p>
</li>
</ol>
<p>After restarting, connect to BL602‚Äôs UART Port at 2 Mbps like so‚Ä¶</p>
<p><strong>For Linux:</strong></p>
<div class="example-wrap"><pre class="language-bash"><code>screen /dev/ttyUSB0 2000000</code></pre></div>
<p><strong>For macOS:</strong> Use CoolTerm (<a href="https://lupyuen.github.io/articles/flash#watch-the-firmware-run">See this</a>)</p>
<p><strong>For Windows:</strong> Use <code>putty</code> (<a href="https://lupyuen.github.io/articles/flash#watch-the-firmware-run">See this</a>)</p>
<p><strong>Alternatively:</strong> Use the Web Serial Terminal (<a href="https://lupyuen.github.io/articles/flash#watch-the-firmware-run">See this</a>)</p>
<p>We‚Äôre ready to enter the Machine Learning Commands into the BL602 Firmware!</p>
<p><a href="https://lupyuen.github.io/articles/flash#watch-the-firmware-run">More details on connecting to BL602</a></p>
<h1 id="machine-learning-in-action"><a href="#machine-learning-in-action">3 Machine Learning in Action</a></h1>
<p>Remember this <strong>wavy curve</strong>?</p>
<p><img src="https://lupyuen.github.io/images/tflite-chart2.jpg" alt="Wavy Curve" /></p>
<p>We wanted to apply <strong>Machine Learning on BL602</strong> to‚Ä¶</p>
<ol>
<li>
<p><strong>Learn</strong> the wavy curve</p>
</li>
<li>
<p><strong>Reproduce</strong> values from the wavy curve</p>
</li>
</ol>
<p>Watch what happens when we enter the <strong>Machine Learning Commands</strong> into the BL602 Firmware.</p>
<h2 id="load-the-model"><a href="#load-the-model">3.1 Load the Model</a></h2>
<p>We enter this command to load BL602‚Äôs ‚Äúbrain‚Äù with knowledge about the wavy curve‚Ä¶</p>
<div class="example-wrap"><pre class="language-text"><code>init</code></pre></div>
<p>(Wow wouldn‚Äôt it be great if we could do this for our School Tests?)</p>
<p>Technically we call this <strong>‚ÄúLoading The TensorFlow Lite Model‚Äù.</strong></p>
<p>The <strong>TensorFlow Lite Model</strong> works like a ‚Äúbrain dump‚Äù or ‚Äúknowledge snapshot‚Äù that tells BL602 everything about the wavy curve.</p>
<p>(How did we create the model? We‚Äôll learn in a while)</p>
<h2 id="run-an-inference"><a href="#run-an-inference">3.2 Run an Inference</a></h2>
<p>Now that BL602 has loaded the TensorFlow Lite Model (and knows everything about the wavy curve), let‚Äôs test it!</p>
<p>This command asks BL602 to <strong>infer the output value</strong> of the wavy curve, given the <strong>input value <code>0.1</code></strong>‚Ä¶</p>
<div class="example-wrap"><pre class="language-text"><code>infer 0.1</code></pre></div>
<p>BL602 responds with the <strong>inferred output value</strong>‚Ä¶</p>
<div class="example-wrap"><pre class="language-text"><code>0.160969</code></pre></div>
<p><img src="https://lupyuen.github.io/images/tflite-chart3.png" alt="Infer Output Value" /></p>
<p>Let‚Äôs test it with two more <strong>input values: <code>0.2</code> and <code>0.3</code></strong>‚Ä¶</p>
<div class="example-wrap"><pre class="language-text"><code># infer 0.2
0.262633

# infer 0.3
0.372770</code></pre></div>
<p>BL602 responds with the <strong>inferred output values: <code>0.262633</code> and <code>0.372770</code></strong></p>
<p>That‚Äôs how we <strong>load a TensorFlow Lite Model</strong> on BL602‚Ä¶ And <strong>run an inference</strong> with the TensorFlow Lite Model!</p>
<ul>
<li><a href="https://youtu.be/cCzUFIdUfio"><strong>Watch the Demo Video on YouTube</strong></a></li>
</ul>
<p><img src="https://lupyuen.github.io/images/tflite-run.png" alt="Run TensorFlow Firmware" /></p>
<h1 id="how-accurate-is-it"><a href="#how-accurate-is-it">4 How Accurate Is It?</a></h1>
<p><em>The wavy curve looks familiar‚Ä¶?</em></p>
<p><img src="https://lupyuen.github.io/images/tflite-chart2.jpg" alt="Wavy Curve" /></p>
<p>Yes it was the <strong>Sine Function</strong> all along!</p>
<blockquote>
<p><strong><code>y = sin( x )</code></strong></p>
</blockquote>
<p>(Input value <code>x</code> is in radians, not degrees)</p>
<p><em>So we were using a TensorFlow Lite Model for the Sine Function?</em></p>
<p>Right! The <strong>‚Äú<code>init</code>‚Äù</strong> command from the previous chapter loads a TensorFlow Lite Model that‚Äôs <strong>trained with the Sine Function.</strong></p>
<p><em>How accurate are the values inferred by the model?</em></p>
<p>Sadly Machine Learning Models are <strong>rarely 100% accurate.</strong></p>
<p>Here‚Äôs a comparison of the <strong>values inferred by the model (left)</strong> and the <strong>actual values (right)</strong>‚Ä¶</p>
<p><img src="https://lupyuen.github.io/images/tflite-compare.jpg" alt="Compare inferred vs actual values" /></p>
<p><em>But we can train the model to be more accurate right?</em></p>
<p>Training the Machine Learning Model on too much data may cause <strong>Overfitting</strong>‚Ä¶</p>
<p>When we vary the input value slightly, the <strong>output value may fluctuate wildly</strong>.</p>
<p>(We definitely don‚Äôt want our LED to glow erratically!)</p>
<p><a href="https://en.wikipedia.org/wiki/Overfitting">More about Overfitting</a></p>
<p><em>Is the model accurate enough?</em></p>
<p>Depends how we‚Äôll be using the model.</p>
<p>For glowing an LED it‚Äôs probably OK to use a Machine Learning Model that‚Äôs accurate to <a href="https://en.wikipedia.org/wiki/Significant_figures"><strong>1 Significant Digit</strong></a>.</p>
<p>We‚Äôll watch the glowing LED in a while!</p>
<p><a href="https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/hello_world">(The TensorFlow Lite Model came from this sample code)</a></p>
<h1 id="how-it-works"><a href="#how-it-works">5 How It Works</a></h1>
<p>Let‚Äôs study the code inside the TensorFlow Lite Firmware for BL602‚Ä¶ To understand how it <strong>loads the TensorFlow Lite Model and runs inferences.</strong></p>
<p>Here are the <strong>C++ Global Variables</strong> needed for TensorFlow Lite: <a href="https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/main_functions.cc#L28-L39"><code>main_functions.cc</code></a></p>
<div class="example-wrap"><pre class="language-c"><code>// Globals for TensorFlow Lite
namespace {
  tflite::ErrorReporter* error_reporter = nullptr;
  const tflite::Model* model = nullptr;
  tflite::MicroInterpreter* interpreter = nullptr;
  TfLiteTensor* input = nullptr;
  TfLiteTensor* output = nullptr;

  constexpr int kTensorArenaSize = 2000;
  uint8_t tensor_arena[kTensorArenaSize];
}</code></pre></div>
<ul>
<li>
<p><strong><code>error_reporter</code></strong> will be used for <strong>printing error messages</strong> to the console</p>
</li>
<li>
<p><strong><code>model</code></strong> is the <strong>TensorFlow Lite Model</strong> that we shall load into memory</p>
</li>
<li>
<p><strong><code>interpreter</code></strong> provides the interface for <strong>running inferences</strong> with the TensorFlow Lite Model</p>
</li>
<li>
<p><strong><code>input</code></strong> is the Tensor that we shall set to specify the <strong>input values</strong> for running an inference</p>
</li>
<li>
<p><strong><code>output</code></strong> is the Tensor that will contain the <strong>output values</strong> after running an inference</p>
</li>
<li>
<p><strong><code>tensor_arena</code></strong> is the <strong>working memory</strong> that will be used by TensorFlow Lite to compute inferences</p>
</li>
</ul>
<p>Now we study the code that populates the above Global Variables.</p>
<h1 id="load-tensorflow-model"><a href="#load-tensorflow-model">6 Load TensorFlow Model</a></h1>
<p>Here‚Äôs the <strong>‚Äú<code>init</code>‚Äù command</strong> for our BL602 Firmware: <a href="https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/demo.c#L21-L24"><code>demo.c</code></a></p>
<div class="example-wrap"><pre class="language-c"><code>/// Command to load the TensorFlow Lite Model (Sine Wave)
static void init(char *buf, int len, int argc, char **argv) {
  load_model();
}</code></pre></div>
<p>The command calls <strong><code>load_model</code></strong> to load the TensorFlow Lite Model: <a href="https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/main_functions.cc#L41-L84"><code>main_functions.cc</code></a></p>
<div class="example-wrap"><pre class="language-c"><code>// Load the TensorFlow Lite Model into Static Memory
void load_model() {
  tflite::InitializeTarget();

  // Set up logging. Google style is to avoid globals or statics because of
  // lifetime uncertainty, but since this has a trivial destructor it&#39;s okay.
  static tflite::MicroErrorReporter micro_error_reporter;
  error_reporter = &amp;micro_error_reporter;</code></pre></div>
<p>Here we <strong>initialise the TensorFlow Lite Library</strong>.</p>
<p>Next we <strong>load the TensorFlow Lite Model</strong>‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>  // Map the model into a usable data structure. This doesn&#39;t involve any
  // copying or parsing, it&#39;s a very lightweight operation.
  model = tflite::GetModel(g_model);
  if (model-&gt;version() != TFLITE_SCHEMA_VERSION) {
    TF_LITE_REPORT_ERROR(error_reporter,
      &quot;Model provided is schema version %d not equal &quot;
      &quot;to supported version %d.&quot;,
      model-&gt;version(), TFLITE_SCHEMA_VERSION);
    return;
  }</code></pre></div>
<p><strong><code>g_model</code></strong> contains the <strong>TensorFlow Lite Model Data</strong>, as defined in <a href="https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/model.cc#L28-L238"><code>model.cc</code></a></p>
<p>We create the <strong>TensorFlow Lite Interpreter</strong> that will be called to run inferences‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>  // This pulls in all the operation implementations we need.
  static tflite::AllOpsResolver resolver;

  // Build an interpreter to run the model with.
  static tflite::MicroInterpreter static_interpreter(
      model, resolver, tensor_arena, kTensorArenaSize, error_reporter);
  interpreter = &amp;static_interpreter;</code></pre></div>
<p>Then we <strong>allocate the working memory</strong> that will be used by the TensorFlow Lite Library to compute inferences‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>  // Allocate memory from the tensor_arena for the model&#39;s tensors.
  TfLiteStatus allocate_status = interpreter-&gt;AllocateTensors();
  if (allocate_status != kTfLiteOk) {
    TF_LITE_REPORT_ERROR(error_reporter, &quot;AllocateTensors() failed&quot;);
    return;
  }</code></pre></div>
<p>Finally we remember the <strong>Input and Output Tensors</strong>‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>  // Obtain pointers to the model&#39;s input and output tensors.
  input = interpreter-&gt;input(0);
  output = interpreter-&gt;output(0);
}</code></pre></div>
<p>Which will be used in the next chapter to run inferences.</p>
<h1 id="run-tensorflow-inference"><a href="#run-tensorflow-inference">7 Run TensorFlow Inference</a></h1>
<p>Earlier we entered this command to <strong>run an inference</strong> with the TensorFlow Lite Model‚Ä¶</p>
<div class="example-wrap"><pre class="language-text"><code># infer 0.1
0.160969</code></pre></div>
<p>Here‚Äôs the <strong>‚Äú<code>infer</code>‚Äù command</strong> in our BL602 Firmware: <a href="https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/demo.c#L26-L37"><code>demo.c</code></a></p>
<div class="example-wrap"><pre class="language-c"><code>/// Command to infer values with TensorFlow Lite Model (Sine Wave)
static void infer(char *buf, int len, int argc, char **argv) {
  //  Convert the argument to float
  if (argc != 2) { printf(&quot;Usage: infer &lt;float&gt;\r\n&quot;); return; }
  float input = atof(argv[1]);</code></pre></div>
<p>To run an inference, the ‚Äú<code>infer</code>‚Äù command accepts <strong>one input value</strong>: a floating-point number.</p>
<p>We pass the floating-point number to the <strong><code>run_inference</code></strong> function‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>  //  Run the inference
  float result = run_inference(input);

  //  Show the result
  printf(&quot;%f\r\n&quot;, result);
}</code></pre></div>
<p>And we <strong>print the result</strong> of the inference. (Another floating-point number)</p>
<p><strong><code>run_inference</code></strong> is defined in <a href="https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/main_functions.cc#L86-L116"><code>main_functions.cc</code></a> ‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>// Run an inference with the loaded TensorFlow Lite Model.
// Return the output value inferred by the model.
float run_inference(
  float x) {  //  Value to be fed into the model

  // Quantize the input from floating-point to integer
  int8_t x_quantized = x / input-&gt;params.scale 
    + input-&gt;params.zero_point;</code></pre></div>
<p>Interesting Fact: Our TensorFlow Lite Model (for Sine Function) actually accepts an <strong>integer input</strong> and produces an <strong>integer output</strong>! (8-bit integers)</p>
<p>(Integer models run more efficiently on microcontrollers)</p>
<p>The code above <strong>converts the floating-point input</strong> to an 8-bit integer.</p>
<p>We pass the <strong>8-bit integer input</strong> to the TensorFlow Lite Model through the <strong>Input Tensor</strong>‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>  // Place the quantized input in the model&#39;s input tensor
  input-&gt;data.int8[0] = x_quantized;</code></pre></div>
<p>Then we <strong>call the interpreter to run the inference</strong> on the TensorFlow Lite Model‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>  // Run inference, and report any error
  TfLiteStatus invoke_status = interpreter-&gt;Invoke();
  if (invoke_status != kTfLiteOk) {
    TF_LITE_REPORT_ERROR(error_reporter, &quot;Invoke failed on x: %f\n&quot;,
      static_cast&lt;double&gt;(x));
    return 0;
  }</code></pre></div>
<p>The 8-bit integer result is <strong>returned through the Output Tensor</strong>‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>  // Obtain the quantized output from model&#39;s output tensor
  int8_t y_quantized = output-&gt;data.int8[0];</code></pre></div>
<p>We <strong>convert the 8-bit integer result</strong> to floating-point‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>  // Dequantize the output from integer to floating-point
  float y = (y_quantized - output-&gt;params.zero_point) 
    * output-&gt;params.scale;

  // Output the results
  return y;
}</code></pre></div>
<p>Finally we <strong>return the floating-point result</strong>.</p>
<p>The code we‚Äôve seen is derived from the <a href="https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/hello_world">TensorFlow Lite Hello World Sample</a>, which is covered here‚Ä¶</p>
<ul>
<li>
<p><a href="https://www.tensorflow.org/lite/microcontrollers/get_started_low_level">‚ÄúTensorFlow Lite: Get started with microcontrollers‚Äù</a></p>
</li>
<li>
<p><a href="https://www.tensorflow.org/lite/microcontrollers/library">‚ÄúTensorFlow Lite: Understand the C++ library‚Äù</a></p>
</li>
</ul>
<h1 id="glow-the-led"><a href="#glow-the-led">8 Glow The LED</a></h1>
<p>As promised, now we <strong>light up the BL602 LED with TensorFlow Lite</strong>!</p>
<p>Here‚Äôs the <strong>‚Äú<code>glow</code>‚Äù</strong> command in our BL602 Firmware: <a href="https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/demo.c#L39-L96"><code>demo.c</code></a></p>
<div class="example-wrap"><pre class="language-c"><code>/// PineCone Blue LED is connected on BL602 GPIO 11
/// TODO: Change the LED GPIO Pin Number for your BL602 board
#define LED_GPIO 11

/// Use PWM Channel 1 to control the LED GPIO.
/// TODO: Select the PWM Channel that matches the LED GPIO
#define PWM_CHANNEL 1

/// Command to glow the LED with values generated by the TensorFlow Lite Model (Sine Wave).
/// We vary the LED brightness with Pulse Widge Modulation:
/// blinking the LED very rapidly with various Duty Cycle settings.
/// See https://lupyuen.github.io/articles/led#from-gpio-to-pulse-width-modulation-pwm
static void glow(char *buf, int len, int argc, char **argv) {
  //  Configure the LED GPIO for PWM
  int rc = bl_pwm_init(
    PWM_CHANNEL,  //  PWM Channel (1) 
    LED_GPIO,     //  GPIO Pin Number (11)
    2000          //  PWM Frequency (2,000 Hz)
  );
  assert(rc == 0);</code></pre></div>
<p>The ‚Äú<code>glow</code>‚Äù command takes the <strong>Output Values</strong> from the TensorFlow Lite Model (Sine Function) and sets the <strong>brightness of the BL602 LED</strong>‚Ä¶</p>
<p><img src="https://lupyuen.github.io/images/tflite-chart2.jpg" alt="Wavy Curve" /></p>
<p>The code above configures the <strong>LED GPIO Pin for PWM Output</strong> at 2,000 cycles per second, by calling the <a href="https://lupyuen.github.io/articles/led#how-it-works-bl602-pwm"><strong>BL602 PWM Hardware Abstraction Layer (HAL)</strong></a>.</p>
<p>(PWM or <strong>Pulse Width Modulation</strong> means that we‚Äôll be pulsing the LED very rapidly at 2,000 times a second, to vary the perceived brightness. <a href="https://lupyuen.github.io/articles/led#from-gpio-to-pulse-width-modulation-pwm">See this</a>)</p>
<p>To set the (perceived) LED Brightness, we set the <strong>PWM Duty Cycle</strong> by calling the BL602 PWM HAL‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>  //  Dim the LED by setting the Duty Cycle to 100%
  rc = bl_pwm_set_duty(
    PWM_CHANNEL,  //  PWM Channel (1) 
    100           //  Duty Cycle (100%)
  );
  assert(rc == 0);</code></pre></div>
<p>Here we set the <strong>Duty Cycle to 100%</strong>, which means that the LED GPIO will be <strong>set to High for 100%</strong> of every PWM Cycle.</p>
<p>Our LED <strong>switches off when the LED GPIO is set to High</strong>. Thus the above code effectively sets the <strong>LED Brightness to 0%</strong>.</p>
<p>But PWM won‚Äôt actually start until we do this‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>  //  Start the PWM, which will blink the LED very rapidly (2,000 times a second)
  rc = bl_pwm_start(PWM_CHANNEL);
  assert(rc == 0);</code></pre></div>
<p>Now that <strong>PWM is started</strong> for our LED GPIO, let‚Äôs vary the LED Brightness‚Ä¶</p>
<ol>
<li>
<p>We do this <strong>4 times</strong></p>
<p>(Giving the glowing LED more time to mesmerise us)</p>
</li>
<li>
<p>We step through the <strong>Input Values from <code>0</code> to <code>6.283</code></strong> (or <code>Pi * 2</code>) at intervals of <code>0.05</code></p>
<p>(Because the TensorFlow Lite Model has been trained on Input Values <code>0</code> to <code>Pi * 2</code>‚Ä¶ One cycle of the Sine Wave)</p>
</li>
</ol>
<div class="example-wrap"><pre class="language-c"><code>  //  Repeat 4 times...
  for (int i = 0; i &lt; 4; i++) {

    //  With input values from 0 to 2 * Pi (stepping by 0.05)...
    for (float input = 0; input &lt; kXrange; input += 0.05) {  //  kXrange is 2 * Pi: 6.283</code></pre></div>
<p>Inside the loops, we <strong>run the TensorFlow Lite inference</strong> with the Input Value (<code>0</code> to <code>6.283</code>)‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>      //  Infer the output value with the TensorFlow Model (Sine Wave)
      float output = run_inference(input);</code></pre></div>
<p>(We‚Äôve seen <code>run_inference</code> in the previous chapter)</p>
<p>The TensorFlow Lite Model (Sine Function) produces an <strong>Output Value that ranges from <code>-1</code> to <code>1</code>.</strong></p>
<p>Negative values are not meaningful for setting the LED Brightness, hence we <strong>multiply the Output Value by itself</strong>‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>      //  Output value has range -1 to 1.
      //  We square the output value to produce range 0 to 1.
      float output_squared = output * output;</code></pre></div>
<p>(Why compute <strong>Output Squared</strong> instead of Output Absolute? Because Sine Squared produces a <strong>smooth curve</strong>, whereas Sine Absolute creates a sharp beak)</p>
<p>Next we set the <strong>Duty Cycle to the Output Value Squared</strong>, scaled to 100%‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>      //  Set the brightness (Duty Cycle) of the PWM LED to the 
      //  output value squared, scaled to 100%
      rc = bl_pwm_set_duty(
        PWM_CHANNEL,                //  PWM Channel (1) 
        (1 - output_squared) * 100  //  Duty Cycle (0% to 100%)
      );
      assert(rc == 0);</code></pre></div>
<p>We <strong>flip the LED Brightness</strong> (1 - Output Squared) because‚Ä¶</p>
<ul>
<li>
<p>Duty Cycle = <strong>0%</strong> means <strong>100%</strong> brightness</p>
</li>
<li>
<p>Duty Cycle = <strong>100%</strong> means <strong>0%</strong> brightness</p>
</li>
</ul>
<p>After setting the LED Brightness, we <strong>sleep for 100 milliseconds</strong>‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>      //  Sleep 100 milliseconds
      time_delay(                //  Sleep by number of ticks (from NimBLE Porting Layer)
        time_ms_to_ticks32(100)  //  Convert 100 milliseconds to ticks (from NimBLE Porting Layer)
      );
    }
  }</code></pre></div>
<p><a href="https://lupyuen.github.io/articles/lora2#multitask-with-nimble-porting-layer">(More about NimBLE Porting Layer)</a></p>
<p>And we repeat both loops.</p>
<p>At the end of the command, we <strong>turn off the PWM</strong> for LED GPIO‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>  //  Stop the PWM, which will stop blinking the LED
  rc = bl_pwm_stop(PWM_CHANNEL);
  assert(rc == 0);
}</code></pre></div>
<p>Let‚Äôs run this!</p>
<p><img src="https://lupyuen.github.io/images/tflite-glow.png" alt="Glowing the LED with TensorFlow Lite" /></p>
<h1 id="glowing-machine-learning-in-action"><a href="#glowing-machine-learning-in-action">9 Glowing Machine Learning in Action</a></h1>
<ol>
<li>
<p>Start the <strong>BL602 Firmware for TensorFlow Lite <code>sdk_app_tflite</code></strong></p>
<p>(As described earlier)</p>
</li>
<li>
<p>Enter this command to <strong>load the TensorFlow Lite Model</strong>‚Ä¶</p>
<div class="example-wrap"><pre class="language-text"><code>init</code></pre></div>
<p>(We‚Äôve seen the ‚Äú<code>init</code>‚Äù command earlier)</p>
</li>
<li>
<p>Then enter this command to <strong>glow the LED with the TensorFlow Lite Model</strong>‚Ä¶</p>
<div class="example-wrap"><pre class="language-text"><code>glow</code></pre></div>
<p>(Yep the ‚Äú<code>glow</code>‚Äù command from the previous chapter)</p>
</li>
<li>
<p>And the BL602 LED glows gently! Brighter and dimmer, brighter and dimmer, ‚Ä¶</p>
<p>(Though the LED flips on abruptly at the end, because we turned off the PWM)</p>
</li>
</ol>
<ul>
<li><a href="https://youtu.be/EFpYJ3qsmEY"><strong>Watch the Demo Video on YouTube</strong></a></li>
</ul>
<p><img src="https://lupyuen.github.io/images/tflite-chart2.jpg" alt="Wavy Curve" /></p>
<p>(Tip: The <strong>Sine Function</strong> is a terrific way to do things <strong>smoothly and continuously</strong>! Because the derivative of <code>sin(x)</code> is <code>cos(x)</code>, another smooth curve! And the derivative of <code>cos(x)</code> is <code>-sin(x)</code>‚Ä¶ Wow!)</p>
<h1 id="train-tensorflow-model"><a href="#train-tensorflow-model">10 Train TensorFlow Model</a></h1>
<p><img src="https://lupyuen.github.io/images/tflite-meme.jpg" alt="Creating a TensorFlow Lite Model won‚Äôt be easy" /></p>
<p>Sorry Padme, it won‚Äôt be easy to <strong>create and train</strong> a TensorFlow Lite Model.</p>
<p>But let‚Äôs quickly run through the steps‚Ä¶</p>
<p><em>Where is the TensorFlow Lite Model defined?</em></p>
<p><strong><code>g_model</code></strong> contains the <strong>TensorFlow Lite Model Data</strong>, as defined in <a href="https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/model.cc#L28-L238"><code>model.cc</code></a> ‚Ä¶</p>
<div class="example-wrap"><pre class="language-c"><code>// Automatically created from a TensorFlow Lite flatbuffer using the command:
//   xxd -i model.tflite &gt; model.cc
// This is a standard TensorFlow Lite model file that has been converted into a
// C data array, so it can be easily compiled into a binary for devices that
// don&#39;t have a file system.
alignas(8) const unsigned char g_model[] = {
  0x1c, 0x00, 0x00, 0x00, 0x54, 0x46, 0x4c, 0x33, 0x14, 0x00, 0x20, 0x00,
  0x1c, 0x00, 0x18, 0x00, 0x14, 0x00, 0x10, 0x00, 0x0c, 0x00, 0x00, 0x00,
  ...
  0x00, 0x00, 0x00, 0x09};
const int g_model_len = 2488;</code></pre></div>
<p>The TensorFlow Lite Model (2,488 bytes) is stored in BL602‚Äôs <strong>XIP Flash ROM</strong>.</p>
<p>This gives the TensorFlow Lite Library more <strong>RAM to run Tensor Computations</strong> for inferencing.</p>
<p>(Remember <strong><code>tensor_arena</code></strong>?)</p>
<p><em>Can we create and train this model on BL602?</em></p>
<p>Sorry Padme nope.</p>
<p>Training a TensorFlow Lite Model requires <strong>Python</strong>. Thus we need a Linux, macOS or Windows computer.</p>
<p>Here‚Äôs the <strong>Python Jupyter Notebook</strong> for training the TensorFlow Lite Model that we have used‚Ä¶</p>
<ul>
<li><a href="https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb?authuser=0">‚ÄúHello World: Jupyter Notebook on Google Colaboratory‚Äù</a></li>
</ul>
<p>Check out the docs on <strong>training and converting TensorFlow Lite Models</strong>‚Ä¶</p>
<ul>
<li>
<p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/train/README.md">‚ÄúTensorFlow Lite: Hello World Training‚Äù</a></p>
</li>
<li>
<p><a href="https://www.tensorflow.org/lite/microcontrollers/build_convert">‚ÄúTensorFlow Lite: Build and convert models‚Äù</a></p>
</li>
</ul>
<h1 id="what-else-can-tensorflow-do"><a href="#what-else-can-tensorflow-do">11 What Else Can TensorFlow Do?</a></h1>
<p>Even though we‚Äôve used TensorFlow Lite for a trivial task (glowing an LED)‚Ä¶ There are <strong>so many possible applications</strong>!</p>
<ol>
<li>
<p>PineCone BL602 Board has a <strong>3-in-1 LED: Red + Green + Blue</strong>.</p>
<p>We could control all 3 LEDs and glow them in a dazzling, multicolour way!</p>
<p>(The TensorFlow Lite Model would probably produce an Output Tensor that contains 3 Output Values)</p>
</li>
<li>
<p>Light up an LED when BL602 <strong>detects my face</strong>.</p>
<p>We could stream the <strong>2D Image Data from a Camera Module</strong> to the TensorFlow Lite Model.</p>
<p><a href="https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/person_detection">Check out the sample code</a></p>
</li>
<li>
<p>Recognise <strong>spoken words and phrases</strong>.</p>
<p>By streaming the <strong>Audio Data from a Microphone</strong> to the TensorFlow Lite Model.</p>
<p><a href="https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/micro_speech">Check out the sample code</a></p>
</li>
<li>
<p>Recognise <strong>motion gestures</strong>.</p>
<p>By streaming the <strong>Motion Data from an Accelerometer</strong> to the TensorFlow Lite Model.</p>
<p><a href="https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/magic_wand">Check out the sample code</a></p>
</li>
</ol>
<h1 id="whats-next"><a href="#whats-next">12 What‚Äôs Next</a></h1>
<p>This has been a super quick tour of TensorFlow Lite.</p>
<p>I hope to see many more <strong>fun and interesting Machine Learning apps</strong> on BL602 and other RISC-V micrcontrollers!</p>
<p>For the next article I shall head back to <a href="https://lupyuen.github.io/articles/rust"><strong>Rust on BL602</strong></a>‚Ä¶ And explain how we create <a href="https://github.com/lupyuen/bl602-rust-wrapper"><strong>Rust Wrappers for the entire BL602 IoT SDK</strong></a>, including GPIO, UART, I2C, SPI, ADC, DAC, LVGL, LoRa, TensorFlow, ‚Ä¶</p>
<p>Stay Tuned!</p>
<ul>
<li>
<p><a href="https://github.com/sponsors/lupyuen">Sponsor me a coffee</a></p>
</li>
<li>
<p><a href="https://www.reddit.com/r/RISCV/comments/o4u9e7/machine_learning_on_riscv_bl602_with_tensorflow/">Discuss this article on Reddit</a></p>
</li>
<li>
<p><a href="https://lupyuen.github.io/articles/book">Read ‚ÄúThe RISC-V BL602 Book‚Äù</a></p>
</li>
<li>
<p><a href="https://lupyuen.github.io">Check out my articles</a></p>
</li>
<li>
<p><a href="https://lupyuen.github.io/rss.xml">RSS Feed</a></p>
</li>
</ul>
<p><em>Got a question, comment or suggestion? Create an Issue or submit a Pull Request here‚Ä¶</em></p>
<p><a href="https://github.com/lupyuen/lupyuen.github.io/blob/master/src/tflite.md"><code>lupyuen.github.io/src/tflite.md</code></a></p>
<h1 id="notes"><a href="#notes">13 Notes</a></h1>
<ol>
<li>This article is the expanded version of <a href="https://twitter.com/MisterTechBlog/status/1402531760764641280">this Twitter Thread</a></li>
</ol>
<h1 id="appendix-porting-tensorflow-to-bl602"><a href="#appendix-porting-tensorflow-to-bl602">14 Appendix: Porting TensorFlow to BL602</a></h1>
<p>In this chapter we discuss the changes we made when <strong>porting TensorFlow Lite to BL602</strong>.</p>
<h2 id="source-repositories"><a href="#source-repositories">14.1 Source Repositories</a></h2>
<p>TensorFlow Lite on BL602 is split across two repositories‚Ä¶</p>
<ol>
<li>
<p><strong>TensorFlow Lite Firmware: <code>sdk_app_tflite</code></strong></p>
<p>This <code>tflite</code> branch of BL602 IoT SDK‚Ä¶</p>
<p><a href="https://github.com/lupyuen/bl_iot_sdk/tree/master">github.com/lupyuen/bl_iot_sdk/tree/master</a></p>
<p>Contains the TensorFlow Lite Firmware at‚Ä¶</p>
<p><a href="https://github.com/lupyuen/bl_iot_sdk/tree/master/customer_app/sdk_app_tflite">customer_app/sdk_app_tflite</a></p>
</li>
<li>
<p><strong>TensorFlow Lite Library: <code>tflite-bl602</code></strong></p>
<p>This TensorFlow Lite Library‚Ä¶</p>
<p><a href="https://github.com/lupyuen/tflite-bl602">github.com/lupyuen/tflite-bl602</a></p>
<p>Should be checked out inside the above BL602 IoT SDK at this folder‚Ä¶</p>
<p><code>components/3rdparty/tflite-bl602</code></p>
</li>
</ol>
<p>When we clone the BL602 IoT SDK recursively‚Ä¶</p>
<div class="example-wrap"><pre class="language-bash"><code># Download the master branch of lupyuen&#39;s bl_iot_sdk
git clone --recursive --branch master https://github.com/lupyuen/bl_iot_sdk</code></pre></div>
<p>The TensorFlow Lite Library <code>tflite-bl602</code> will be automatically cloned to <code>components/3rdparty</code></p>
<p>(Because <code>tflite-bl602</code> is a Git Submodule of <code>bl_iot_sdk</code>)</p>
<h2 id="makefiles"><a href="#makefiles">14.2 Makefiles</a></h2>
<p>TensorFlow Lite builds with <a href="https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/tools/make/Makefile">its own Makefile</a>.</p>
<p>However we‚Äôre using the Makefiles from BL602 IoT SDK, so we merged the TensorFlow Lite build steps into these BL602 Makefiles‚Ä¶</p>
<p><strong>TensorFlow Lite Library Makefiles</strong></p>
<ul>
<li>
<p><a href="https://github.com/lupyuen/tflite-bl602/blob/main/bouffalo.mk"><code>tflite-bl602/bouffalo.mk</code></a></p>
</li>
<li>
<p><a href="https://github.com/lupyuen/tflite-bl602/blob/main/component.mk"><code>tflite-bl602/component.mk</code></a></p>
</li>
</ul>
<p><strong>TensorFlow Lite Firmware Makefiles</strong></p>
<ul>
<li>
<p><a href="https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/Makefile"><code>sdk_app_tflite/Makefile</code></a></p>
</li>
<li>
<p><a href="https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/bouffalo.mk"><code>sdk_app_tflite/bouffalo.mk</code></a></p>
</li>
</ul>
<p>The changes are described in the following sections.</p>
<h2 id="source-folders"><a href="#source-folders">14.3 Source Folders</a></h2>
<p>Here are the source folders that we compile for the TensorFlow Lite Firmware‚Ä¶</p>
<p>From <a href="https://github.com/lupyuen/tflite-bl602/blob/main/bouffalo.mk"><code>tflite-bl602/bouffalo.mk</code></a> and <a href="https://github.com/lupyuen/tflite-bl602/blob/main/component.mk"><code>tflite-bl602/component.mk</code></a></p>
<div class="example-wrap"><pre class="language-text"><code># Include Folders
# TODO: Sync with bouffalo.mk and component.mk
COMPONENT_ADD_INCLUDEDIRS := \
  tensorflow/.. \
  tensorflow/lite/micro/tools/make/downloads/flatbuffers/include \
  tensorflow/lite/micro/tools/make/downloads/gemmlowp \
  tensorflow/lite/micro/tools/make/downloads/ruy

# Source Folders
# TODO: Sync with bouffalo.mk and component.mk
COMPONENT_SRCDIRS := \
  tensorflow/lite/c \
  tensorflow/lite/core/api \
  tensorflow/lite/kernels \
  tensorflow/lite/kernels/internal \
  tensorflow/lite/micro \
  tensorflow/lite/micro/kernels \
  tensorflow/lite/micro/memory_planner \
  tensorflow/lite/schema</code></pre></div>
<p>The source folders are specified in both <a href="https://github.com/lupyuen/tflite-bl602/blob/main/bouffalo.mk"><code>bouffalo.mk</code></a> and <a href="https://github.com/lupyuen/tflite-bl602/blob/main/component.mk"><code>component.mk</code></a>. We should probably specify the source folders in a common Makefile instead‚Ä¶</p>
<p><img src="https://lupyuen.github.io/images/tflite-source.png" alt="Source Folders" /></p>
<h2 id="compiler-flags"><a href="#compiler-flags">14.4 Compiler Flags</a></h2>
<p>Here are the GCC Compiler Flags for TensorFlow Lite Library: <a href="https://github.com/lupyuen/tflite-bl602/blob/main/bouffalo.mk#L28-L49"><code>tflite-bl602/bouffalo.mk</code></a></p>
<div class="example-wrap"><pre class="language-text"><code># Define the GCC compiler options:
# CFLAGS for C compiler, CPPFLAGS for C++ compiler

# Use global C math functions instead of std library.
# See tensorflow/lite/kernels/internal/cppmath.h
CFLAGS   += -DTF_LITE_USE_GLOBAL_CMATH_FUNCTIONS
CPPFLAGS += -DTF_LITE_USE_GLOBAL_CMATH_FUNCTIONS

# Use std::min instead of std::fmin
# See tensorflow/lite/kernels/internal/min.h
CFLAGS   += -DTF_LITE_USE_GLOBAL_MIN
CPPFLAGS += -DTF_LITE_USE_GLOBAL_MIN

# Use std::max instead of std::fmax
# See tensorflow/lite/kernels/internal/max.h
CFLAGS   += -DTF_LITE_USE_GLOBAL_MAX
CPPFLAGS += -DTF_LITE_USE_GLOBAL_MAX

# Use Static Memory instead of Heap Memory
# See tensorflow/lite/kernels/internal/types.h
CFLAGS   += -DTF_LITE_STATIC_MEMORY
CPPFLAGS += -DTF_LITE_STATIC_MEMORY</code></pre></div>
<p>And here are the flags for TensorFlow Lite Firmware: <a href="https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/bouffalo.mk#L28-L41"><code>sdk_app_tflite/bouffalo.mk</code></a></p>
<div class="example-wrap"><pre class="language-text"><code># Define the GCC compiler options:
# CFLAGS for C compiler, CPPFLAGS for C++ compiler
# See additional options at components/3rdparty/tflite-bl602/bouffalo.mk

# Use Static Memory instead of Heap Memory
# See components/3rdparty/tflite-bl602/tensorflow/lite/kernels/internal/types.h
CFLAGS   += -DTF_LITE_STATIC_MEMORY
CPPFLAGS += -DTF_LITE_STATIC_MEMORY

# Don&#39;t use Thread-Safe Initialisation for C++ Static Variables.
# This fixes the missing symbols __cxa_guard_acquire and __cxa_guard_release.
# Note: This assumes that we will not init C++ static variables in multiple tasks.
# See https://alex-robenko.gitbook.io/bare_metal_cpp/compiler_output/static
CPPFLAGS += -fno-threadsafe-statics</code></pre></div>
<p><img src="https://lupyuen.github.io/images/tflite-cppflags2.png" alt="Compiler Flags" /></p>
<p><strong>TF_LITE_USE_GLOBAL_CMATH_FUNCTIONS</strong> is needed because we use the global C Math Functions instead of the C++ <code>std</code> library‚Ä¶</p>
<p><img src="https://lupyuen.github.io/images/tflite-math.png" alt="" /></p>
<p><strong>TF_LITE_STATIC_MEMORY</strong> is needed because we use Static Memory instead of Dynamic Memory (<code>new</code> and <code>delete</code>)‚Ä¶</p>
<p><img src="https://lupyuen.github.io/images/tflite-undefined2.png" alt="Undefined new and delete" /></p>
<p><strong>no-threadsafe-statics</strong> is needed to disable <strong>Thread-Safe Initialisation</strong> for C++ Static Variables. This fixes the missing symbols <code>__cxa_guard_acquire</code> and <code>__cxa_guard_release</code>.</p>
<p>Note: This assumes that we will not init C++ static variables in multiple tasks.
<a href="https://alex-robenko.gitbook.io/bare_metal_cpp/compiler_output/static">(See this)</a></p>
<p><img src="https://lupyuen.github.io/images/tflite-initstatic.png" alt="Disable Thread-Safe Initialisation" /></p>
<p>Note that <strong><code>CPPFLAGS</code></strong> (for C++ compiler) should be defined in <a href="https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/bouffalo.mk"><code>sdk_app_tflite/bouffalo.mk</code></a> instead of <a href="https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/Makefile"><code>sdk_app_tflite/Makefile</code></a>‚Ä¶</p>
<p><img src="https://lupyuen.github.io/images/tflite-cppflags.png" alt="Compiler Flags" /></p>
<h2 id="download-libraries"><a href="#download-libraries">14.5 Download Libraries</a></h2>
<p>TensorFlow Lite needs <strong>4 External Libraries</strong> for its build‚Ä¶</p>
<ol>
<li>
<p><a href="https://github.com/google/flatbuffers"><strong><code>flatbuffers</code></strong></a>: Serialisation Library (similar to Protocol Buffers). TensorFlow Lite Models are encoded in the <code>flatbuffers</code> format.</p>
</li>
<li>
<p><a href="https://pigweed.googlesource.com/pigweed/pigweed"><strong><code>pigweed</code></strong></a>: Embedded Libraries <a href="https://opensource.googleblog.com/2020/03/pigweed-collection-of-embedded-libraries.html">(See this)</a></p>
</li>
<li>
<p><a href="https://github.com/google/gemmlowp"><strong><code>gemmlowp</code></strong></a>: Small self-contained low-precision General Matrix Multiplication library. Input and output matrix entries are integers on at most 8 bits.</p>
</li>
<li>
<p><a href="https://github.com/google/ruy"><strong><code>ruy</code></strong></a>: Matrix Multiplication Library for neural network inference engines. Supports floating-point and 8-bit integer-quantized matrices.</p>
</li>
</ol>
<p>To download <a href="https://github.com/google/flatbuffers"><strong><code>flatbuffers</code></strong></a> and <a href="https://pigweed.googlesource.com/pigweed/pigweed"><strong><code>pigweed</code></strong></a>, we copied these steps from <a href="https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/tools/make/Makefile#L509-L542">TensorFlow Lite‚Äôs Makefile</a> to <a href="https://github.com/lupyuen/tflite-bl602/blob/main/bouffalo.mk#L51-L112"><code>tflite-bl602/bouffalo.mk</code></a> ‚Ä¶</p>
<div class="example-wrap"><pre class="language-text"><code># TensorFlow Makefile
# Based on https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/tools/make/Makefile#L509-L542

# root directory of tensorflow
TENSORFLOW_ROOT := 
MAKEFILE_DIR := $(BL60X_SDK_PATH)/components/3rdparty/tflite-bl602/tensorflow/lite/micro/tools/make

# For some invocations of the makefile, it is useful to avoid downloads. This
# can be achieved by explicitly passing in DISABLE_DOWNLOADS=true on the command
# line. Note that for target-specific downloads (e.g. CMSIS) there will need to
# be corresponding checking in the respecitve included makefiles (e.g.
# ext_libs/cmsis_nn.inc)
DISABLE_DOWNLOADS :=

ifneq ($(DISABLE_DOWNLOADS), true)
  # The download scripts require that the downloads directory already exist for
  # improved error checking. To accomodate that, we first create a downloads
  # directory.
  $(shell mkdir -p ${MAKEFILE_DIR}/downloads)

  # Directly download the flatbuffers library.
  DOWNLOAD_RESULT := $(shell $(MAKEFILE_DIR)/flatbuffers_download.sh ${MAKEFILE_DIR}/downloads)
  ifneq ($(DOWNLOAD_RESULT), SUCCESS)
    $(error Something went wrong with the flatbuffers download: $(DOWNLOAD_RESULT))
  endif

  DOWNLOAD_RESULT := $(shell $(MAKEFILE_DIR)/pigweed_download.sh ${MAKEFILE_DIR}/downloads)
  ifneq ($(DOWNLOAD_RESULT), SUCCESS)
    $(error Something went wrong with the pigweed download: $(DOWNLOAD_RESULT))
  endif</code></pre></div>
<p><img src="https://lupyuen.github.io/images/tflite-gemmlowp2.png" alt="Download gemmlowp" /></p>
<p>Unfortunately these steps dont‚Äôt work for downloading <a href="https://github.com/google/gemmlowp"><strong><code>gemmlowp</code></strong></a> and <a href="https://github.com/google/ruy"><strong><code>ruy</code></strong></a> ‚Ä¶</p>
<div class="example-wrap"><pre class="language-text"><code>  # TODO: Fix third-party downloads
  include $(MAKEFILE_DIR)/third_party_downloads.inc
  THIRD_PARTY_DOWNLOADS :=
  $(eval $(call add_third_party_download,$(GEMMLOWP_URL),$(GEMMLOWP_MD5),gemmlowp,))
  $(eval $(call add_third_party_download,$(RUY_URL),$(RUY_MD5),ruy,))
  $(eval $(call add_third_party_download,$(PERSON_MODEL_URL),$(PERSON_MODEL_MD5),person_model_grayscale,))
  RESULT := $(shell $(MAKEFILE_DIR)/person_detection_int8_download.sh ${MAKEFILE_DIR}/downloads $(CO_PROCESSOR))
  ifneq ($(RESULT), SUCCESS)
    $(error Something went wrong with the person detection int8 model download: $(RESULT))
  endif  
  ...
endif

# TODO: Fix third-party downloads
# Create rules for downloading third-party dependencies.
THIRD_PARTY_TARGETS :=
$(foreach DOWNLOAD,$(THIRD_PARTY_DOWNLOADS),$(eval $(call create_download_rule,$(DOWNLOAD))))
third_party_downloads: $(THIRD_PARTY_TARGETS)</code></pre></div>
<p>So we download <a href="https://github.com/google/gemmlowp"><strong><code>gemmlowp</code></strong></a> and <a href="https://github.com/google/ruy"><strong><code>ruy</code></strong></a> ourselves: <a href="https://github.com/lupyuen/tflite-bl602/blob/main/bouffalo.mk#L93-L104"><code>tflite-bl602/bouffalo.mk</code></a></p>
<div class="example-wrap"><pre class="language-text"><code>  # Added GEMMLOWP, RUY downloads
  # TODO: Use the download rules in helper_functions.inc
  RESULT := $(shell $(MAKEFILE_DIR)/download_and_extract.sh $(GEMMLOWP_URL) $(GEMMLOWP_MD5) ${MAKEFILE_DIR}/downloads/gemmlowp)

  # TODO: Check results of download
  # ifneq ($(RESULT), SUCCESS)
  #   $(error Something went wrong with the GEMMLOWP download: $(RESULT))
  # endif

  RESULT := $(shell $(MAKEFILE_DIR)/download_and_extract.sh $(RUY_URL) $(RUY_MD5) ${MAKEFILE_DIR}/downloads/ruy)
  # TODO: Check results of download
  # ifneq ($(RESULT), SUCCESS)
  #   $(error Something went wrong with the RUY download: $(RESULT))
  # endif
endif</code></pre></div>
<p><code>GEMMLOWP_URL</code> and <code>RUY_URL</code> are defined in <a href="https://github.com/lupyuen/tflite-bl602/blob/main/tensorflow/lite/micro/tools/make/third_party_downloads.inc"><code>third_party_downloads</code></a> ‚Ä¶</p>
<div class="example-wrap"><pre class="language-text"><code>GEMMLOWP_URL := &quot;https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip&quot;
GEMMLOWP_MD5 := &quot;7e8191b24853d75de2af87622ad293ba&quot;

RUY_URL=&quot;https://github.com/google/ruy/archive/d37128311b445e758136b8602d1bbd2a755e115d.zip&quot;
RUY_MD5=&quot;abf7a91eb90d195f016ebe0be885bb6e&quot;</code></pre></div>
<p><img src="https://lupyuen.github.io/images/tflite-ruy.png" alt="Download ruy" /></p>
<h2 id="but-not-on-windows-msys"><a href="#but-not-on-windows-msys">14.6 But Not On Windows MSYS</a></h2>
<p>TensorFlow Lite builds OK on Linux and macOS. But on Windows MSYS it shows this error‚Ä¶</p>
<div class="example-wrap"><pre class="language-text"><code>/d/a/bl_iot_sdk/bl_iot_sdk/components/3rdparty/tflite-bl602/tensorflow/lite/micro/tools/make/
flatbuffers_download.sh: line 102: 
unzip: command not found
*** Something went wrong with the flatbuffers download: .
Stop.
...
D:/a/bl_iot_sdk/bl_iot_sdk/customer_app/sdk_app_tflite/sdk_app_tflite
main_functions.cc:19:10: 
fatal error: tensorflow/lite/micro/all_ops_resolver.h: 
No such file or directory
#include &quot;tensorflow/lite/micro/all_ops_resolver.h&quot;
^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2021-06-22T13:40:25.9719870Z compilation terminated.</code></pre></div>
<p><a href="https://github.com/lupyuen/bl_iot_sdk/blob/master/.github/workflows/build.yml#L42-L62">(From this GitHub Actions Workflow: <code>build.yml</code>)</a></p>
<p>The build for Windows MSYS probably needs <code>unzip</code> to be installed.</p>
<h2 id="global-destructor"><a href="#global-destructor">14.7 Global Destructor</a></h2>
<p>C++ Programs (like TensorFlow Lite) need a <strong>Global Destructor <code>__dso_handle</code></strong> that points to the Static C++ Objects that will be destroyed when the program is terminated. <a href="https://alex-robenko.gitbook.io/bare_metal_cpp/compiler_output/static#custom-destructors">(See this)</a></p>
<p>We won‚Äôt be destroying any Static C++ Objects. (Because our firmware doesn‚Äôt have a shutdown command) Hence we set the Global Destructor to null: <a href="https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/demo.c#L105-L107"><code>sdk_app_tflite/demo.c</code></a></p>
<div class="example-wrap"><pre class="language-c"><code>/// Global Destructor for C++, which we&#39;re not using.
/// See https://alex-robenko.gitbook.io/bare_metal_cpp/compiler_output/static#custom-destructors
void *__dso_handle = NULL;</code></pre></div>
<p><img src="https://lupyuen.github.io/images/tflite-dsohandle.png" alt="Global Destructor for C++" /></p>
<h2 id="math-overflow"><a href="#math-overflow">14.8 Math Overflow</a></h2>
<p><strong><code>__math_oflowf</code></strong> is called by C++ Programs to handle <strong>Floating-Point Math Overflow</strong>.</p>
<p>For BL602 we halt with an <strong>Assertion Failure</strong> when Math Overflow occurs: <a href="https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/demo.c#L98-L103"><code>sdk_app_tflite/demo.c</code></a></p>
<div class="example-wrap"><pre class="language-c"><code>/// TODO: Handle math overflow.
float __math_oflowf (uint32_t sign) {
    assert(false);  //  For now, we halt when there is a math overflow
    //  Previously: return xflowf (sign, 0x1p97f);
    //  From https://code.woboq.org/userspace/glibc/sysdeps/ieee754/flt-32/math_errf.c.html#__math_oflowf
}</code></pre></div><h2 id="excluded-files"><a href="#excluded-files">14.9 Excluded Files</a></h2>
<p>These two files were <strong>excluded from the build</strong> because of compile errors‚Ä¶</p>
<ol>
<li>
<p><a href="https://github.com/lupyuen/tflite-bl602/blob/main/tensorflow/lite/micro/kernels/space_to_depth.cc"><strong><code>space_to_depth.cc</code></strong></a></p>
</li>
<li>
<p><a href="https://github.com/lupyuen/tflite-bl602/blob/main/tensorflow/lite/micro/kernels/space_to_depth_test.cc"><strong><code>space_to_depth_test.cc</code></strong></a></p>
</li>
</ol>
<p><a href="https://github.com/lupyuen/tflite-bl602/commit/d0afe5b344d898c0bb461206fbbbf234731d7d77#diff-08b6d122af82deb31fb40975dd2c6786943ef3f16787d6b8a288d474de3093b7">See the changes</a></p>
<h2 id="optimise-tensorflow"><a href="#optimise-tensorflow">14.10 Optimise TensorFlow</a></h2>
<p>TensorFlow Lite for BL602 was compiled for a RISC-V CPU <strong>without any special hardware optimisation</strong>.</p>
<p>For CPUs with <a href="https://en.m.wikipedia.org/wiki/Vector_processor"><strong>Vector Processing</strong></a> or <a href="https://en.m.wikipedia.org/wiki/Digital_signal_processor"><strong>Digital Signal Processing</strong></a> Instructions, we may optimise TensorFlow Lite by executing these instructions.</p>
<p>Check out this doc on <strong>TensorFlow Lite optimisation</strong>‚Ä¶</p>
<ul>
<li><a href="https://www.tensorflow.org/lite/microcontrollers/library#optimized_kernels"><strong>‚ÄúTensorFlow Lite: Optimised Kernels‚Äù</strong></a></li>
</ul>
<p>This doc explains how TensorFlow Lite was <strong>optimised for <a href="https://github.com/SpinalHDL/VexRiscv">VexRISCV</a></strong>‚Ä¶</p>
<ul>
<li><a href="https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/kernels/vexriscv/doc/DepthwiseConv2D_int8.md"><strong>‚ÄúTensorFlow Lite: DepthwiseConv2D for VexRISCV‚Äù</strong></a></li>
</ul>
<p><img src="https://lupyuen.github.io/images/tflite-build.png" alt="Build OK" /></p>

    
</body>
</html>