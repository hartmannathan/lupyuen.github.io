# Machine Learning on RISC-V BL602 with TensorFlow Lite

📝 _22 Jun 2021_

How a Human teaches a Machine to light up an LED...

> _Human:_ Hello Machine, please light up the LED in a fun and interesting way.

> _Machine:_ OK I shall light up the LED: on - off - on -off - on - off...

![On - Off - On - Off](https://lupyuen.github.io/images/tflite-chart1.jpg)

> _Human:_ That's not very fun and interesting.

> _Machine:_ OK Hooman... Define fun and interesting.

> _Human:_ Make the LED glow gently brighter and dimmer, brighter and dimmer, and so on.

> _Machine:_ Like a wavy curve? Please teach me to draw a wavy curve.

> _Human:_ Like this...

![Wavy Curve](https://lupyuen.github.io/images/tflite-chart2.jpg)

> _Machine:_ OK I have been trained. I shall now use my trained model to infer the values of the wavy curve. And light up the LED in a fun and interesting way.

-   [__Watch the Demo Video on YouTube__](https://youtu.be/EFpYJ3qsmEY)

This sounds like Science Fiction... But __this is possible today!__

(Except for the polite banter)

Read on to learn how __Machine Learning (TensorFlow Lite)__ makes this possible on the [__BL602 RISC-V + WiFi SoC__](https://lupyuen.github.io/articles/pinecone).

# TensorFlow Lite Library

Remember in our story...

1. Our Machine __learns to draw a wavy curve__

1. Our Machine __reproduces the wavy curve__ (to light up the LED)

To accomplish (1) and (2) on BL602, we shall use an open-source __Machine Learning__ library: [__TensorFlow Lite for Microcontrollers__](https://www.tensorflow.org/lite/microcontrollers)

_What's a Tensor?_

Remember these from our Math Textbook? __Scalar, Vector and Matrix__

![Scalar, Vector, Matrix](https://lupyuen.github.io/images/tflite-matrix.png)

[(From TensorFlow Guide)](https://www.tensorflow.org/guide/tensor)

When we extend a Matrix from 2D to 3D, we get a __Tensor With 3 Axes__...

![Tensor with 3 and 4 Axes](https://lupyuen.github.io/images/tflite-tensor.png)

And yes we can have a __Tensor With 4 or More Axes__!

__Tensors With Multiple Dimensions__ are really useful for crunching the numbers needed for Machine Learning.

That's how the TensorFlow library works: __Computing lots of Tensors__.

(Fortunately we won't need to compute any Tensors ourselves... The library does everything for us)

[More about Tensors](https://www.tensorflow.org/guide/tensor)

_Why is the library named TensorFlow?_

Because it doesn't drip, it flows 😂

But seriously... In Machine Learning we push lots of numbers __(Tensors)__ through various math functions over specific paths __(Dataflow Graphs)__.

That's why it's named __"TensorFlow"__

(Yes it sounds like the Neural Network in our brain)

[More about TensorFlow](https://en.m.wikipedia.org/wiki/TensorFlow)

_What's the "Lite" version of TensorFlow?_

TensorFlow normally runs on powerful servers to perform Machine Learning tasks. (Like Speech Recognition and Image Recognition)

We're using __TensorFlow Lite__, which is __optimised for microcontrollers__...

1.  Works on microcontrollers with __limited RAM__

    (Including Arduino, Arm and ESP32)

1.  Uses __Static Memory__ instead of Dynamic Memory (Heap)

1.  But it only supports __Basic Models__ of Machine Learning

Today we shall study the TensorFlow Lite library that has been ported to BL602...

-   [__`tflite-bl602` TensorFlow Lite Library for BL602__](https://github.com/lupyuen/tflite-bl602)

# TensorFlow Lite Firmware

Let's build, flash and run the TensorFlow Lite Firmware for BL602... And watch Machine Learning in action!

## Build the Firmware

Download the Firmware Binary File __`sdk_app_tflite.bin`__ from...

-  [__Binary Release of `sdk_app_tflite`__](https://github.com/lupyuen/bl_iot_sdk/releases/tag/v10.0.0)

Alternatively, we may build the Firmware Binary File `sdk_app_tflite.bin` from the [source code](https://github.com/lupyuen/bl_iot_sdk/tree/master/customer_app/sdk_app_tflite)...

```bash
## Download the master branch of lupyuen's bl_iot_sdk
git clone --recursive --branch master https://github.com/lupyuen/bl_iot_sdk

## TODO: Change this to the full path of bl_iot_sdk
export BL60X_SDK_PATH=$PWD/bl_iot_sdk
export CONFIG_CHIP_NAME=BL602

## Build the firmware
cd bl_iot_sdk/customer_app/sdk_app_tflite
make

## For WSL: Copy the firmware to /mnt/c/blflash, which refers to c:\blflash in Windows
mkdir /mnt/c/blflash
cp build_out/sdk_app_tflite.bin /mnt/c/blflash
```

[More details on building bl_iot_sdk](https://lupyuen.github.io/articles/pinecone#building-firmware)

## Flash the Firmware

Follow these steps to install `blflash`...

1.  [__"Install rustup"__](https://lupyuen.github.io/articles/flash#install-rustup)

1.  [__"Download and build blflash"__](https://lupyuen.github.io/articles/flash#download-and-build-blflash)

We assume that our Firmware Binary File `sdk_app_tflite.bin` has been copied to the `blflash` folder.

Set BL602 to __Flashing Mode__ and restart the board...

__For PineCone:__

1.  Set the __PineCone Jumper (IO 8)__ to the __`H` Position__ [(Like this)](https://lupyuen.github.io/images/pinecone-jumperh.jpg)

1.  Press the Reset Button

__For BL10:__

1.  Connect BL10 to the USB port

1.  Press and hold the __D8 Button (GPIO 8)__

1.  Press and release the __EN Button (Reset)__

1.  Release the D8 Button

__For [Ai-Thinker Ai-WB2](https://docs.ai-thinker.com/en/wb2), Pinenut and MagicHome BL602:__

1.  Disconnect the board from the USB Port

1.  Connect __GPIO 8__ to __3.3V__

1.  Reconnect the board to the USB port

Enter these commands to flash `sdk_app_tflite.bin` to BL602 over UART...

```bash
## For Linux:
blflash flash build_out/sdk_app_tflite.bin \
    --port /dev/ttyUSB0

## For macOS:
blflash flash build_out/sdk_app_tflite.bin \
    --port /dev/tty.usbserial-1420 \
    --initial-baud-rate 230400 \
    --baud-rate 230400

## For Windows: Change COM5 to the BL602 Serial Port
blflash flash c:\blflash\sdk_app_tflite.bin --port COM5
```

(For WSL: Do this under plain old Windows CMD, not WSL, because blflash needs to access the COM port)

[More details on flashing firmware](https://lupyuen.github.io/articles/flash#flash-the-firmware)

## Run the Firmware

Set BL602 to __Normal Mode__ (Non-Flashing) and restart the board...

__For PineCone:__

1.  Set the __PineCone Jumper (IO 8)__ to the __`L` Position__ [(Like this)](https://lupyuen.github.io/images/pinecone-jumperl.jpg)

1.  Press the Reset Button

__For BL10:__

1.  Press and release the __EN Button (Reset)__

__For [Ai-Thinker Ai-WB2](https://docs.ai-thinker.com/en/wb2), Pinenut and MagicHome BL602:__

1.  Disconnect the board from the USB Port

1.  Connect __GPIO 8__ to __GND__

1.  Reconnect the board to the USB port

After restarting, connect to BL602's UART Port at 2 Mbps like so...

__For Linux:__

```bash
screen /dev/ttyUSB0 2000000
```

__For macOS:__ Use CoolTerm ([See this](https://lupyuen.github.io/articles/flash#watch-the-firmware-run))

__For Windows:__ Use `putty` ([See this](https://lupyuen.github.io/articles/flash#watch-the-firmware-run))

__Alternatively:__ Use the Web Serial Terminal ([See this](https://lupyuen.github.io/articles/flash#watch-the-firmware-run))

We're ready to enter the Machine Learning Commands into the BL602 Firmware!

[More details on connecting to BL602](https://lupyuen.github.io/articles/flash#watch-the-firmware-run)

# Machine Learning in Action

Remember this __wavy curve__?

![Wavy Curve](https://lupyuen.github.io/images/tflite-chart2.jpg)

We wanted to apply __Machine Learning on BL602__ to...

1.  __Learn__ the wavy curve

1.  __Reproduce__ values from the wavy curve

Watch what happens when we enter the __Machine Learning Commands__ into the BL602 Firmware.

## Load the Model

We enter this command to load BL602's "brain" with knowledge about the wavy curve...

```text
init
```

(Wow wouldn't it be great if we could do this for our School Tests?)

Technically we call this __"Loading The TensorFlow Lite Model".__

The __TensorFlow Lite Model__ works like a "brain dump" or "knowledge snapshot" that tells BL602 everything about the wavy curve.

(How did we create the model? We'll learn in a while)

## Run an Inference

Now that BL602 has loaded the TensorFlow Lite Model (and knows everything about the wavy curve), let's test it!

This command asks BL602 to __infer the output value__ of the wavy curve, given the __input value `0.1`__...

```text
infer 0.1
```

BL602 responds with the __inferred output value__...

```text
0.160969
```

![Infer Output Value](https://lupyuen.github.io/images/tflite-chart3.png)

Let's test it with two more __input values: `0.2` and `0.3`__...

```text
## infer 0.2
0.262633

## infer 0.3
0.372770
```

BL602 responds with the __inferred output values: `0.262633` and `0.372770`__

That's how we __load a TensorFlow Lite Model__ on BL602... And __run an inference__ with the TensorFlow Lite Model!

-   [__Watch the Demo Video on YouTube__](https://youtu.be/cCzUFIdUfio)

![Run TensorFlow Firmware](https://lupyuen.github.io/images/tflite-run.png)

# How Accurate Is It?

_The wavy curve looks familiar...?_

![Wavy Curve](https://lupyuen.github.io/images/tflite-chart2.jpg)

Yes it was the __Sine Function__ all along!

> __`y = sin( x )`__

(Input value `x` is in radians, not degrees)

_So we were using a TensorFlow Lite Model for the Sine Function?_

Right! The __"`init`"__ command from the previous chapter loads a TensorFlow Lite Model that's __trained with the Sine Function.__

_How accurate are the values inferred by the model?_

Sadly Machine Learning Models are __rarely 100% accurate.__

Here's a comparison of the __values inferred by the model (left)__ and the __actual values (right)__...

![Compare inferred vs actual values](https://lupyuen.github.io/images/tflite-compare.jpg)

_But we can train the model to be more accurate right?_

Training the Machine Learning Model on too much data may cause __Overfitting__...

When we vary the input value slightly, the __output value may fluctuate wildly__.

(We definitely don't want our LED to glow erratically!)

[More about Overfitting](https://en.wikipedia.org/wiki/Overfitting)

_Is the model accurate enough?_

Depends how we'll be using the model.

For glowing an LED it's probably OK to use a Machine Learning Model that's accurate to [__1 Significant Digit__](https://en.wikipedia.org/wiki/Significant_figures).

We'll watch the glowing LED in a while!

[(The TensorFlow Lite Model came from this sample code)](https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/hello_world)

# How It Works

Let's study the code inside the TensorFlow Lite Firmware for BL602... To understand how it __loads the TensorFlow Lite Model and runs inferences.__

Here are the __C++ Global Variables__ needed for TensorFlow Lite: [`main_functions.cc`](https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/main_functions.cc#L28-L39)

```c
// Globals for TensorFlow Lite
namespace {
  tflite::ErrorReporter* error_reporter = nullptr;
  const tflite::Model* model = nullptr;
  tflite::MicroInterpreter* interpreter = nullptr;
  TfLiteTensor* input = nullptr;
  TfLiteTensor* output = nullptr;

  constexpr int kTensorArenaSize = 2000;
  uint8_t tensor_arena[kTensorArenaSize];
}
```

-   __`error_reporter`__ will be used for __printing error messages__ to the console

-   __`model`__ is the __TensorFlow Lite Model__ that we shall load into memory

-   __`interpreter`__ provides the interface for __running inferences__ with the TensorFlow Lite Model

-   __`input`__ is the Tensor that we shall set to specify the __input values__ for running an inference

-   __`output`__ is the Tensor that will contain the __output values__ after running an inference

-   __`tensor_arena`__ is the __working memory__ that will be used by TensorFlow Lite to compute inferences

Now we study the code that populates the above Global Variables.

# Load TensorFlow Model

Here's the __"`init`" command__ for our BL602 Firmware: [`demo.c`](https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/demo.c#L21-L24)

```c
/// Command to load the TensorFlow Lite Model (Sine Wave)
static void init(char *buf, int len, int argc, char **argv) {
  load_model();
}
```

The command calls __`load_model`__ to load the TensorFlow Lite Model: [`main_functions.cc`](https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/main_functions.cc#L41-L84)

```c
// Load the TensorFlow Lite Model into Static Memory
void load_model() {
  tflite::InitializeTarget();

  // Set up logging. Google style is to avoid globals or statics because of
  // lifetime uncertainty, but since this has a trivial destructor it's okay.
  static tflite::MicroErrorReporter micro_error_reporter;
  error_reporter = &micro_error_reporter;
```

Here we __initialise the TensorFlow Lite Library__.

Next we __load the TensorFlow Lite Model__...

```c
  // Map the model into a usable data structure. This doesn't involve any
  // copying or parsing, it's a very lightweight operation.
  model = tflite::GetModel(g_model);
  if (model->version() != TFLITE_SCHEMA_VERSION) {
    TF_LITE_REPORT_ERROR(error_reporter,
      "Model provided is schema version %d not equal "
      "to supported version %d.",
      model->version(), TFLITE_SCHEMA_VERSION);
    return;
  }
```

__`g_model`__ contains the __TensorFlow Lite Model Data__, as defined in [`model.cc`](https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/model.cc#L28-L238)

We create the __TensorFlow Lite Interpreter__ that will be called to run inferences...

```c
  // This pulls in all the operation implementations we need.
  static tflite::AllOpsResolver resolver;

  // Build an interpreter to run the model with.
  static tflite::MicroInterpreter static_interpreter(
      model, resolver, tensor_arena, kTensorArenaSize, error_reporter);
  interpreter = &static_interpreter;
```

Then we __allocate the working memory__ that will be used by the TensorFlow Lite Library to compute inferences...

```c
  // Allocate memory from the tensor_arena for the model's tensors.
  TfLiteStatus allocate_status = interpreter->AllocateTensors();
  if (allocate_status != kTfLiteOk) {
    TF_LITE_REPORT_ERROR(error_reporter, "AllocateTensors() failed");
    return;
  }
```

Finally we remember the __Input and Output Tensors__...

```c
  // Obtain pointers to the model's input and output tensors.
  input = interpreter->input(0);
  output = interpreter->output(0);
}
```

Which will be used in the next chapter to run inferences.

# Run TensorFlow Inference

Earlier we entered this command to __run an inference__ with the TensorFlow Lite Model...

```text
## infer 0.1
0.160969
```

Here's the __"`infer`" command__ in our BL602 Firmware: [`demo.c`](https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/demo.c#L26-L37)

```c
/// Command to infer values with TensorFlow Lite Model (Sine Wave)
static void infer(char *buf, int len, int argc, char **argv) {
  //  Convert the argument to float
  if (argc != 2) { printf("Usage: infer <float>\r\n"); return; }
  float input = atof(argv[1]);
```

To run an inference, the "`infer`" command accepts __one input value__: a floating-point number.

We pass the floating-point number to the __`run_inference`__ function...

```c
  //  Run the inference
  float result = run_inference(input);

  //  Show the result
  printf("%f\r\n", result);
}
```

And we __print the result__ of the inference. (Another floating-point number)

__`run_inference`__ is defined in [`main_functions.cc`](https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/main_functions.cc#L86-L116) ...

```c
// Run an inference with the loaded TensorFlow Lite Model.
// Return the output value inferred by the model.
float run_inference(
  float x) {  //  Value to be fed into the model

  // Quantize the input from floating-point to integer
  int8_t x_quantized = x / input->params.scale 
    + input->params.zero_point;
```

Interesting Fact: Our TensorFlow Lite Model (for Sine Function) actually accepts an __integer input__ and produces an __integer output__! (8-bit integers)

(Integer models run more efficiently on microcontrollers)

The code above __converts the floating-point input__ to an 8-bit integer.

We pass the __8-bit integer input__ to the TensorFlow Lite Model through the __Input Tensor__...

```c
  // Place the quantized input in the model's input tensor
  input->data.int8[0] = x_quantized;
```

Then we __call the interpreter to run the inference__ on the TensorFlow Lite Model...

```c
  // Run inference, and report any error
  TfLiteStatus invoke_status = interpreter->Invoke();
  if (invoke_status != kTfLiteOk) {
    TF_LITE_REPORT_ERROR(error_reporter, "Invoke failed on x: %f\n",
      static_cast<double>(x));
    return 0;
  }
```

The 8-bit integer result is __returned through the Output Tensor__...

```c
  // Obtain the quantized output from model's output tensor
  int8_t y_quantized = output->data.int8[0];
```

We __convert the 8-bit integer result__ to floating-point...

```c
  // Dequantize the output from integer to floating-point
  float y = (y_quantized - output->params.zero_point) 
    * output->params.scale;

  // Output the results
  return y;
}
```

Finally we __return the floating-point result__.

The code we've seen is derived from the [TensorFlow Lite Hello World Sample](https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/hello_world), which is covered here...

-   ["TensorFlow Lite: Get started with microcontrollers"](https://www.tensorflow.org/lite/microcontrollers/get_started_low_level)

-   ["TensorFlow Lite: Understand the C++ library"](https://www.tensorflow.org/lite/microcontrollers/library)

# Glow The LED

As promised, now we __light up the BL602 LED with TensorFlow Lite__!

Here's the __"`glow`"__ command in our BL602 Firmware: [`demo.c`](https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/demo.c#L39-L96)

```c
/// PineCone Blue LED is connected on BL602 GPIO 11
/// TODO: Change the LED GPIO Pin Number for your BL602 board
#define LED_GPIO 11

/// Use PWM Channel 1 to control the LED GPIO.
/// TODO: Select the PWM Channel that matches the LED GPIO
#define PWM_CHANNEL 1

/// Command to glow the LED with values generated by the TensorFlow Lite Model (Sine Wave).
/// We vary the LED brightness with Pulse Widge Modulation:
/// blinking the LED very rapidly with various Duty Cycle settings.
/// See https://lupyuen.github.io/articles/led#from-gpio-to-pulse-width-modulation-pwm
static void glow(char *buf, int len, int argc, char **argv) {
  //  Configure the LED GPIO for PWM
  int rc = bl_pwm_init(
    PWM_CHANNEL,  //  PWM Channel (1) 
    LED_GPIO,     //  GPIO Pin Number (11)
    2000          //  PWM Frequency (2,000 Hz)
  );
  assert(rc == 0);
```

The "`glow`" command takes the __Output Values__ from the TensorFlow Lite Model (Sine Function) and sets the __brightness of the BL602 LED__...

![Wavy Curve](https://lupyuen.github.io/images/tflite-chart2.jpg)

The code above configures the __LED GPIO Pin for PWM Output__ at 2,000 cycles per second, by calling the [__BL602 PWM Hardware Abstraction Layer (HAL)__](https://lupyuen.github.io/articles/led#how-it-works-bl602-pwm).

(PWM or __Pulse Width Modulation__ means that we'll be pulsing the LED very rapidly at 2,000 times a second, to vary the perceived brightness. [See this](https://lupyuen.github.io/articles/led#from-gpio-to-pulse-width-modulation-pwm))

To set the (perceived) LED Brightness, we set the __PWM Duty Cycle__ by calling the BL602 PWM HAL...

```c
  //  Dim the LED by setting the Duty Cycle to 100%
  rc = bl_pwm_set_duty(
    PWM_CHANNEL,  //  PWM Channel (1) 
    100           //  Duty Cycle (100%)
  );
  assert(rc == 0);
```

Here we set the __Duty Cycle to 100%__, which means that the LED GPIO will be __set to High for 100%__ of every PWM Cycle.

Our LED __switches off when the LED GPIO is set to High__. Thus the above code effectively sets the __LED Brightness to 0%__.

But PWM won't actually start until we do this...

```c
  //  Start the PWM, which will blink the LED very rapidly (2,000 times a second)
  rc = bl_pwm_start(PWM_CHANNEL);
  assert(rc == 0);
```

Now that __PWM is started__ for our LED GPIO, let's vary the LED Brightness...

1.  We do this __4 times__

    (Giving the glowing LED more time to mesmerise us)

1.  We step through the __Input Values from `0` to `6.283`__ (or `Pi * 2`) at intervals of `0.05`

    (Because the TensorFlow Lite Model has been trained on Input Values `0` to `Pi * 2`... One cycle of the Sine Wave)

```c
  //  Repeat 4 times...
  for (int i = 0; i < 4; i++) {

    //  With input values from 0 to 2 * Pi (stepping by 0.05)...
    for (float input = 0; input < kXrange; input += 0.05) {  //  kXrange is 2 * Pi: 6.283
```

Inside the loops, we __run the TensorFlow Lite inference__ with the Input Value (`0` to `6.283`)...

```c
      //  Infer the output value with the TensorFlow Model (Sine Wave)
      float output = run_inference(input);
```

(We've seen `run_inference` in the previous chapter)

The TensorFlow Lite Model (Sine Function) produces an __Output Value that ranges from `-1` to `1`.__

Negative values are not meaningful for setting the LED Brightness, hence we __multiply the Output Value by itself__...

```c
      //  Output value has range -1 to 1.
      //  We square the output value to produce range 0 to 1.
      float output_squared = output * output;
```

(Why compute __Output Squared__ instead of Output Absolute? Because Sine Squared produces a __smooth curve__, whereas Sine Absolute creates a sharp beak)

Next we set the __Duty Cycle to the Output Value Squared__, scaled to 100%...

```c
      //  Set the brightness (Duty Cycle) of the PWM LED to the 
      //  output value squared, scaled to 100%
      rc = bl_pwm_set_duty(
        PWM_CHANNEL,                //  PWM Channel (1) 
        (1 - output_squared) * 100  //  Duty Cycle (0% to 100%)
      );
      assert(rc == 0);
```

We __flip the LED Brightness__ (1 - Output Squared) because...

-  Duty Cycle = __0%__ means __100%__ brightness

-  Duty Cycle = __100%__ means __0%__ brightness

After setting the LED Brightness, we __sleep for 100 milliseconds__...

```c
      //  Sleep 100 milliseconds
      time_delay(                //  Sleep by number of ticks (from NimBLE Porting Layer)
        time_ms_to_ticks32(100)  //  Convert 100 milliseconds to ticks (from NimBLE Porting Layer)
      );
    }
  }
```

[(More about NimBLE Porting Layer)](https://lupyuen.github.io/articles/lora2#multitask-with-nimble-porting-layer)

And we repeat both loops.

At the end of the command, we __turn off the PWM__ for LED GPIO...

```c
  //  Stop the PWM, which will stop blinking the LED
  rc = bl_pwm_stop(PWM_CHANNEL);
  assert(rc == 0);
}
```

Let's run this!

![Glowing the LED with TensorFlow Lite](https://lupyuen.github.io/images/tflite-glow.png)

# Glowing Machine Learning in Action

1.  Start the __BL602 Firmware for TensorFlow Lite `sdk_app_tflite`__

    (As described earlier)

1.  Enter this command to __load the TensorFlow Lite Model__...

    ```text
    init
    ```

    (We've seen the "`init`" command earlier)

1.  Then enter this command to __glow the LED with the TensorFlow Lite Model__...

    ```text
    glow
    ```

    (Yep the "`glow`" command from the previous chapter)

1.  And the BL602 LED glows gently! Brighter and dimmer, brighter and dimmer, ...

    (Though the LED flips on abruptly at the end, because we turned off the PWM)

-   [__Watch the Demo Video on YouTube__](https://youtu.be/EFpYJ3qsmEY)

![Wavy Curve](https://lupyuen.github.io/images/tflite-chart2.jpg)

(Tip: The __Sine Function__ is a terrific way to do things __smoothly and continuously__! Because the derivative of `sin(x)` is `cos(x)`, another smooth curve! And the derivative of `cos(x)` is `-sin(x)`... Wow!)

# Train TensorFlow Model

![Creating a TensorFlow Lite Model won't be easy](https://lupyuen.github.io/images/tflite-meme.jpg)

Sorry Padme, it won't be easy to __create and train__ a TensorFlow Lite Model.

But let's quickly run through the steps...

_Where is the TensorFlow Lite Model defined?_

__`g_model`__ contains the __TensorFlow Lite Model Data__, as defined in [`model.cc`](https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/model.cc#L28-L238) ...

```c
// Automatically created from a TensorFlow Lite flatbuffer using the command:
//   xxd -i model.tflite > model.cc
// This is a standard TensorFlow Lite model file that has been converted into a
// C data array, so it can be easily compiled into a binary for devices that
// don't have a file system.
alignas(8) const unsigned char g_model[] = {
  0x1c, 0x00, 0x00, 0x00, 0x54, 0x46, 0x4c, 0x33, 0x14, 0x00, 0x20, 0x00,
  0x1c, 0x00, 0x18, 0x00, 0x14, 0x00, 0x10, 0x00, 0x0c, 0x00, 0x00, 0x00,
  ...
  0x00, 0x00, 0x00, 0x09};
const int g_model_len = 2488;
```

The TensorFlow Lite Model (2,488 bytes) is stored in BL602's __XIP Flash ROM__.

This gives the TensorFlow Lite Library more __RAM to run Tensor Computations__ for inferencing.

(Remember __`tensor_arena`__?)

_Can we create and train this model on BL602?_

Sorry Padme nope.

Training a TensorFlow Lite Model requires __Python__. Thus we need a Linux, macOS or Windows computer.

Here's the __Python Jupyter Notebook__ for training the TensorFlow Lite Model that we have used...

-   ["Hello World: Jupyter Notebook on Google Colaboratory"](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb?authuser=0)

Check out the docs on __training and converting TensorFlow Lite Models__...

-   ["TensorFlow Lite: Hello World Training"](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/train/README.md)

-   ["TensorFlow Lite: Build and convert models"](https://www.tensorflow.org/lite/microcontrollers/build_convert)

# What Else Can TensorFlow Do?

Even though we've used TensorFlow Lite for a trivial task (glowing an LED)... There are __so many possible applications__!

1.  PineCone BL602 Board has a __3-in-1 LED: Red + Green + Blue__.

    We could control all 3 LEDs and glow them in a dazzling, multicolour way!

    (The TensorFlow Lite Model would probably produce an Output Tensor that contains 3 Output Values)

1.  Light up an LED when BL602 __detects my face__.

    We could stream the __2D Image Data from a Camera Module__ to the TensorFlow Lite Model.

    [Check out the sample code](https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/person_detection)

1.  Recognise __spoken words and phrases__.

    By streaming the __Audio Data from a Microphone__ to the TensorFlow Lite Model.

    [Check out the sample code](https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/micro_speech)

1.  Recognise __motion gestures__.

    By streaming the __Motion Data from an Accelerometer__ to the TensorFlow Lite Model.

    [Check out the sample code](https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/magic_wand)

# What's Next

This has been a super quick tour of TensorFlow Lite.

I hope to see many more __fun and interesting Machine Learning apps__ on BL602 and other RISC-V micrcontrollers!

For the next article I shall head back to [__Rust on BL602__](https://lupyuen.github.io/articles/rust)... And explain how we create [__Rust Wrappers for the entire BL602 IoT SDK__](https://github.com/lupyuen/bl602-rust-wrapper), including GPIO, UART, I2C, SPI, ADC, DAC, LVGL, LoRa, TensorFlow, ...

Stay Tuned!

-   [Sponsor me a coffee](https://lupyuen.github.io/articles/sponsor)

-   [Discuss this article on Reddit](https://www.reddit.com/r/RISCV/comments/o4u9e7/machine_learning_on_riscv_bl602_with_tensorflow/)

-   [Read "The RISC-V BL602 Book"](https://lupyuen.github.io/articles/book)

-   [Check out my articles](https://lupyuen.github.io)

-   [RSS Feed](https://lupyuen.github.io/rss.xml)

_Got a question, comment or suggestion? Create an Issue or submit a Pull Request here..._

[`lupyuen.github.io/src/tflite.md`](https://github.com/lupyuen/lupyuen.github.io/blob/master/src/tflite.md)

# Notes

1.  This article is the expanded version of [this Twitter Thread](https://twitter.com/MisterTechBlog/status/1402531760764641280)

# Appendix: Porting TensorFlow to BL602

In this chapter we discuss the changes we made when __porting TensorFlow Lite to BL602__.

## Source Repositories

TensorFlow Lite on BL602 is split across two repositories...

1.  __TensorFlow Lite Firmware: `sdk_app_tflite`__

    This `tflite` branch of BL602 IoT SDK...

    [github.com/lupyuen/bl_iot_sdk/tree/master](https://github.com/lupyuen/bl_iot_sdk/tree/master)

    Contains the TensorFlow Lite Firmware at...

    [customer_app/sdk_app_tflite](https://github.com/lupyuen/bl_iot_sdk/tree/master/customer_app/sdk_app_tflite)

1.  __TensorFlow Lite Library: `tflite-bl602`__

    This TensorFlow Lite Library...

    [github.com/lupyuen/tflite-bl602](https://github.com/lupyuen/tflite-bl602)

    Should be checked out inside the above BL602 IoT SDK at this folder...

    `components/3rdparty/tflite-bl602`

When we clone the BL602 IoT SDK recursively...

```bash
## Download the master branch of lupyuen's bl_iot_sdk
git clone --recursive --branch master https://github.com/lupyuen/bl_iot_sdk
```

The TensorFlow Lite Library `tflite-bl602` will be automatically cloned to `components/3rdparty`

(Because `tflite-bl602` is a Git Submodule of `bl_iot_sdk`)

## Makefiles

TensorFlow Lite builds with [its own Makefile](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/tools/make/Makefile).

However we're using the Makefiles from BL602 IoT SDK, so we merged the TensorFlow Lite build steps into these BL602 Makefiles...

__TensorFlow Lite Library Makefiles__

-   [`tflite-bl602/bouffalo.mk`](https://github.com/lupyuen/tflite-bl602/blob/main/bouffalo.mk)

-   [`tflite-bl602/component.mk`](https://github.com/lupyuen/tflite-bl602/blob/main/component.mk)

__TensorFlow Lite Firmware Makefiles__

-   [`sdk_app_tflite/Makefile`](https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/Makefile)

-   [`sdk_app_tflite/bouffalo.mk`](https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/bouffalo.mk)

The changes are described in the following sections.

## Source Folders

Here are the source folders that we compile for the TensorFlow Lite Firmware...

From [`tflite-bl602/bouffalo.mk`](https://github.com/lupyuen/tflite-bl602/blob/main/bouffalo.mk) and [`tflite-bl602/component.mk`](https://github.com/lupyuen/tflite-bl602/blob/main/component.mk)

```text
## Include Folders
## TODO: Sync with bouffalo.mk and component.mk
COMPONENT_ADD_INCLUDEDIRS := \
  tensorflow/.. \
  tensorflow/lite/micro/tools/make/downloads/flatbuffers/include \
  tensorflow/lite/micro/tools/make/downloads/gemmlowp \
  tensorflow/lite/micro/tools/make/downloads/ruy

## Source Folders
## TODO: Sync with bouffalo.mk and component.mk
COMPONENT_SRCDIRS := \
  tensorflow/lite/c \
  tensorflow/lite/core/api \
  tensorflow/lite/kernels \
  tensorflow/lite/kernels/internal \
  tensorflow/lite/micro \
  tensorflow/lite/micro/kernels \
  tensorflow/lite/micro/memory_planner \
  tensorflow/lite/schema
```

The source folders are specified in both [`bouffalo.mk`](https://github.com/lupyuen/tflite-bl602/blob/main/bouffalo.mk) and [`component.mk`](https://github.com/lupyuen/tflite-bl602/blob/main/component.mk). We should probably specify the source folders in a common Makefile instead...

![Source Folders](https://lupyuen.github.io/images/tflite-source.png)

## Compiler Flags

Here are the GCC Compiler Flags for TensorFlow Lite Library: [`tflite-bl602/bouffalo.mk`](https://github.com/lupyuen/tflite-bl602/blob/main/bouffalo.mk#L28-L49)

```text
## Define the GCC compiler options:
## CFLAGS for C compiler, CPPFLAGS for C++ compiler

## Use global C math functions instead of std library.
## See tensorflow/lite/kernels/internal/cppmath.h
CFLAGS   += -DTF_LITE_USE_GLOBAL_CMATH_FUNCTIONS
CPPFLAGS += -DTF_LITE_USE_GLOBAL_CMATH_FUNCTIONS

## Use std::min instead of std::fmin
## See tensorflow/lite/kernels/internal/min.h
CFLAGS   += -DTF_LITE_USE_GLOBAL_MIN
CPPFLAGS += -DTF_LITE_USE_GLOBAL_MIN

## Use std::max instead of std::fmax
## See tensorflow/lite/kernels/internal/max.h
CFLAGS   += -DTF_LITE_USE_GLOBAL_MAX
CPPFLAGS += -DTF_LITE_USE_GLOBAL_MAX

## Use Static Memory instead of Heap Memory
## See tensorflow/lite/kernels/internal/types.h
CFLAGS   += -DTF_LITE_STATIC_MEMORY
CPPFLAGS += -DTF_LITE_STATIC_MEMORY
```

And here are the flags for TensorFlow Lite Firmware: [`sdk_app_tflite/bouffalo.mk`](https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/bouffalo.mk#L28-L41)

```text
## Define the GCC compiler options:
## CFLAGS for C compiler, CPPFLAGS for C++ compiler
## See additional options at components/3rdparty/tflite-bl602/bouffalo.mk

## Use Static Memory instead of Heap Memory
## See components/3rdparty/tflite-bl602/tensorflow/lite/kernels/internal/types.h
CFLAGS   += -DTF_LITE_STATIC_MEMORY
CPPFLAGS += -DTF_LITE_STATIC_MEMORY

## Don't use Thread-Safe Initialisation for C++ Static Variables.
## This fixes the missing symbols __cxa_guard_acquire and __cxa_guard_release.
## Note: This assumes that we will not init C++ static variables in multiple tasks.
## See https://alex-robenko.gitbook.io/bare_metal_cpp/compiler_output/static
CPPFLAGS += -fno-threadsafe-statics
```

![Compiler Flags](https://lupyuen.github.io/images/tflite-cppflags2.png)

__TF_LITE_USE_GLOBAL_CMATH_FUNCTIONS__ is needed because we use the global C Math Functions instead of the C++ `std` library...

![](https://lupyuen.github.io/images/tflite-math.png)

__TF_LITE_STATIC_MEMORY__ is needed because we use Static Memory instead of Dynamic Memory (`new` and `delete`)...

![Undefined new and delete](https://lupyuen.github.io/images/tflite-undefined2.png)

__no-threadsafe-statics__ is needed to disable __Thread-Safe Initialisation__ for C++ Static Variables. This fixes the missing symbols `__cxa_guard_acquire` and `__cxa_guard_release`.

Note: This assumes that we will not init C++ static variables in multiple tasks.
[(See this)](https://alex-robenko.gitbook.io/bare_metal_cpp/compiler_output/static)

![Disable Thread-Safe Initialisation](https://lupyuen.github.io/images/tflite-initstatic.png)

Note that __`CPPFLAGS`__ (for C++ compiler) should be defined in [`sdk_app_tflite/bouffalo.mk`](https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/bouffalo.mk) instead of [`sdk_app_tflite/Makefile`](https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/Makefile)...

![Compiler Flags](https://lupyuen.github.io/images/tflite-cppflags.png)

## Download Libraries

TensorFlow Lite needs __4 External Libraries__ for its build...

1.  [__`flatbuffers`__](https://github.com/google/flatbuffers): Serialisation Library (similar to Protocol Buffers). TensorFlow Lite Models are encoded in the `flatbuffers` format.

1.  [__`pigweed`__](https://pigweed.googlesource.com/pigweed/pigweed): Embedded Libraries [(See this)](https://opensource.googleblog.com/2020/03/pigweed-collection-of-embedded-libraries.html)

1.  [__`gemmlowp`__](https://github.com/google/gemmlowp): Small self-contained low-precision General Matrix Multiplication library. Input and output matrix entries are integers on at most 8 bits.

1.  [__`ruy`__](https://github.com/google/ruy): Matrix Multiplication Library for neural network inference engines. Supports floating-point and 8-bit integer-quantized matrices.

To download [__`flatbuffers`__](https://github.com/google/flatbuffers) and [__`pigweed`__](https://pigweed.googlesource.com/pigweed/pigweed), we copied these steps from [TensorFlow Lite's Makefile](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/tools/make/Makefile#L509-L542) to [`tflite-bl602/bouffalo.mk`](https://github.com/lupyuen/tflite-bl602/blob/main/bouffalo.mk#L51-L112) ...

```text
## TensorFlow Makefile
## Based on https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/tools/make/Makefile#L509-L542

## root directory of tensorflow
TENSORFLOW_ROOT := 
MAKEFILE_DIR := $(BL60X_SDK_PATH)/components/3rdparty/tflite-bl602/tensorflow/lite/micro/tools/make

## For some invocations of the makefile, it is useful to avoid downloads. This
## can be achieved by explicitly passing in DISABLE_DOWNLOADS=true on the command
## line. Note that for target-specific downloads (e.g. CMSIS) there will need to
## be corresponding checking in the respecitve included makefiles (e.g.
## ext_libs/cmsis_nn.inc)
DISABLE_DOWNLOADS :=

ifneq ($(DISABLE_DOWNLOADS), true)
  ## The download scripts require that the downloads directory already exist for
  ## improved error checking. To accomodate that, we first create a downloads
  ## directory.
  $(shell mkdir -p ${MAKEFILE_DIR}/downloads)

  ## Directly download the flatbuffers library.
  DOWNLOAD_RESULT := $(shell $(MAKEFILE_DIR)/flatbuffers_download.sh ${MAKEFILE_DIR}/downloads)
  ifneq ($(DOWNLOAD_RESULT), SUCCESS)
    $(error Something went wrong with the flatbuffers download: $(DOWNLOAD_RESULT))
  endif

  DOWNLOAD_RESULT := $(shell $(MAKEFILE_DIR)/pigweed_download.sh ${MAKEFILE_DIR}/downloads)
  ifneq ($(DOWNLOAD_RESULT), SUCCESS)
    $(error Something went wrong with the pigweed download: $(DOWNLOAD_RESULT))
  endif
```

![Download gemmlowp](https://lupyuen.github.io/images/tflite-gemmlowp2.png)

Unfortunately these steps dont't work for downloading [__`gemmlowp`__](https://github.com/google/gemmlowp) and [__`ruy`__](https://github.com/google/ruy) ...

```text
  ## TODO: Fix third-party downloads
  include $(MAKEFILE_DIR)/third_party_downloads.inc
  THIRD_PARTY_DOWNLOADS :=
  $(eval $(call add_third_party_download,$(GEMMLOWP_URL),$(GEMMLOWP_MD5),gemmlowp,))
  $(eval $(call add_third_party_download,$(RUY_URL),$(RUY_MD5),ruy,))
  $(eval $(call add_third_party_download,$(PERSON_MODEL_URL),$(PERSON_MODEL_MD5),person_model_grayscale,))
  RESULT := $(shell $(MAKEFILE_DIR)/person_detection_int8_download.sh ${MAKEFILE_DIR}/downloads $(CO_PROCESSOR))
  ifneq ($(RESULT), SUCCESS)
    $(error Something went wrong with the person detection int8 model download: $(RESULT))
  endif  
  ...
endif

## TODO: Fix third-party downloads
## Create rules for downloading third-party dependencies.
THIRD_PARTY_TARGETS :=
$(foreach DOWNLOAD,$(THIRD_PARTY_DOWNLOADS),$(eval $(call create_download_rule,$(DOWNLOAD))))
third_party_downloads: $(THIRD_PARTY_TARGETS)
```

So we download [__`gemmlowp`__](https://github.com/google/gemmlowp) and [__`ruy`__](https://github.com/google/ruy) ourselves: [`tflite-bl602/bouffalo.mk`](https://github.com/lupyuen/tflite-bl602/blob/main/bouffalo.mk#L93-L104)

```text
  ## Added GEMMLOWP, RUY downloads
  ## TODO: Use the download rules in helper_functions.inc
  RESULT := $(shell $(MAKEFILE_DIR)/download_and_extract.sh $(GEMMLOWP_URL) $(GEMMLOWP_MD5) ${MAKEFILE_DIR}/downloads/gemmlowp)

  ## TODO: Check results of download
  ## ifneq ($(RESULT), SUCCESS)
  ##   $(error Something went wrong with the GEMMLOWP download: $(RESULT))
  ## endif

  RESULT := $(shell $(MAKEFILE_DIR)/download_and_extract.sh $(RUY_URL) $(RUY_MD5) ${MAKEFILE_DIR}/downloads/ruy)
  ## TODO: Check results of download
  ## ifneq ($(RESULT), SUCCESS)
  ##   $(error Something went wrong with the RUY download: $(RESULT))
  ## endif
endif
```

`GEMMLOWP_URL` and `RUY_URL` are defined in [`third_party_downloads`](https://github.com/lupyuen/tflite-bl602/blob/main/tensorflow/lite/micro/tools/make/third_party_downloads.inc) ...

```text
GEMMLOWP_URL := "https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip"
GEMMLOWP_MD5 := "7e8191b24853d75de2af87622ad293ba"

RUY_URL="https://github.com/google/ruy/archive/d37128311b445e758136b8602d1bbd2a755e115d.zip"
RUY_MD5="abf7a91eb90d195f016ebe0be885bb6e"
```

![Download ruy](https://lupyuen.github.io/images/tflite-ruy.png)

## But Not On Windows MSYS

TensorFlow Lite builds OK on Linux and macOS. But on Windows MSYS it shows this error...

```text
/d/a/bl_iot_sdk/bl_iot_sdk/components/3rdparty/tflite-bl602/tensorflow/lite/micro/tools/make/
flatbuffers_download.sh: line 102: 
unzip: command not found
*** Something went wrong with the flatbuffers download: .
Stop.
...
D:/a/bl_iot_sdk/bl_iot_sdk/customer_app/sdk_app_tflite/sdk_app_tflite
main_functions.cc:19:10: 
fatal error: tensorflow/lite/micro/all_ops_resolver.h: 
No such file or directory
#include "tensorflow/lite/micro/all_ops_resolver.h"
^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2021-06-22T13:40:25.9719870Z compilation terminated.
```

[(From this GitHub Actions Workflow: `build.yml`)](https://github.com/lupyuen/bl_iot_sdk/blob/master/.github/workflows/build.yml#L42-L62)

The build for Windows MSYS probably needs `unzip` to be installed.

## Global Destructor

C++ Programs (like TensorFlow Lite) need a __Global Destructor `__dso_handle`__ that points to the Static C++ Objects that will be destroyed when the program is terminated. [(See this)](https://alex-robenko.gitbook.io/bare_metal_cpp/compiler_output/static#custom-destructors)

We won't be destroying any Static C++ Objects. (Because our firmware doesn't have a shutdown command) Hence we set the Global Destructor to null: [`sdk_app_tflite/demo.c`](https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/demo.c#L105-L107)

```c
/// Global Destructor for C++, which we're not using.
/// See https://alex-robenko.gitbook.io/bare_metal_cpp/compiler_output/static#custom-destructors
void *__dso_handle = NULL;
```

![Global Destructor for C++](https://lupyuen.github.io/images/tflite-dsohandle.png)

## Math Overflow

__`__math_oflowf`__ is called by C++ Programs to handle __Floating-Point Math Overflow__.

For BL602 we halt with an __Assertion Failure__ when Math Overflow occurs: [`sdk_app_tflite/demo.c`](https://github.com/lupyuen/bl_iot_sdk/blob/master/customer_app/sdk_app_tflite/sdk_app_tflite/demo.c#L98-L103)

```c
/// TODO: Handle math overflow.
float __math_oflowf (uint32_t sign) {
    assert(false);  //  For now, we halt when there is a math overflow
    //  Previously: return xflowf (sign, 0x1p97f);
    //  From https://code.woboq.org/userspace/glibc/sysdeps/ieee754/flt-32/math_errf.c.html#__math_oflowf
}
```

## Excluded Files

These two files were __excluded from the build__ because of compile errors...

1.  [__`space_to_depth.cc`__](https://github.com/lupyuen/tflite-bl602/blob/main/tensorflow/lite/micro/kernels/space_to_depth.cc)

1.  [__`space_to_depth_test.cc`__](https://github.com/lupyuen/tflite-bl602/blob/main/tensorflow/lite/micro/kernels/space_to_depth_test.cc)

[See the changes](https://github.com/lupyuen/tflite-bl602/commit/d0afe5b344d898c0bb461206fbbbf234731d7d77#diff-08b6d122af82deb31fb40975dd2c6786943ef3f16787d6b8a288d474de3093b7)

## Optimise TensorFlow

TensorFlow Lite for BL602 was compiled for a RISC-V CPU __without any special hardware optimisation__.

For CPUs with [__Vector Processing__](https://en.m.wikipedia.org/wiki/Vector_processor) or [__Digital Signal Processing__](https://en.m.wikipedia.org/wiki/Digital_signal_processor) Instructions, we may optimise TensorFlow Lite by executing these instructions.

Check out this doc on __TensorFlow Lite optimisation__...

-   [__"TensorFlow Lite: Optimised Kernels"__](https://www.tensorflow.org/lite/microcontrollers/library#optimized_kernels)

This doc explains how TensorFlow Lite was __optimised for [VexRISCV](https://github.com/SpinalHDL/VexRiscv)__...

-   [__"TensorFlow Lite: DepthwiseConv2D for VexRISCV"__](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/kernels/vexriscv/doc/DepthwiseConv2D_int8.md)

![Build OK](https://lupyuen.github.io/images/tflite-build.png)
